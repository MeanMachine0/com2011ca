{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb3c6816",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a1f9a5f2ffe87dcf8348e74f9091b4f5",
     "grade": false,
     "grade_id": "cell-b681404766d89b94",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# COM2011 Machine Learning and Data Science\n",
    "## Course Assessment\n",
    "\n",
    "\n",
    "This course assessment (CA) represents 40% of the overall module assessment.\n",
    "\n",
    "This is an individual exercise and your attention is drawn to the College and University guidelines on collaboration and plagiarism, which are available from the College website. Students are **not allowed** to use Large Language Models (such as ChatGPT, Bard, etc) to generate code for the CA.\n",
    "\n",
    "\n",
    "**Submission information:**\n",
    "1. do not change the name of this notebook, i.e. the notebook file has to be: `ca.ipynb`\n",
    "2. do not add you name or student code in the notebook or in the file name\n",
    "3. do not remove or delete or add any cell in this notebook\n",
    "4. make sure to **remove** and **delete** the `raise NotImplementedError()` under the `# YOUR CODE HERE` and replace it with **your code**: note that if you leave it in the cell you will fail the associated test\n",
    "5. do not remove the function `save_history()` at the end of each cell. This function will save your edit operations on the code in the cell and will be used as proof of work, i.e. proof that you have been working on the questions assigned\n",
    "6. work always in the cells provided when developing your implementation, i.e. do not work on another notebook or with programming environments that do not operate on this notebook.\n",
    "7. when you are finished debugging **remove** all code that is not part of the function definition, i.e. leave only the clean function implementation in the cell: do not leave debugging `print` statements in the functions, and do not leave function invocations on test inputs\n",
    "8. make sure that the exeecution of the cell **does not produce any type of output**: the exeecution of the cell should only define the desired functions\n",
    "9. before the final submission run the function `check_and_prepare_for_submission()` in the last cell of the notebook: this function will create a zip archive called `com2011ca.zip` which contains your notebook and the folder `proof_of_work`.\n",
    "10. Submit only the file `com2011ca.zip`\n",
    "\n",
    "\n",
    "**Evaluation criteria:**\n",
    "\n",
    "Each question asks for one or more functions to be implemented. \n",
    "\n",
    "- Each function is awarded a number of marks. \n",
    "- One or more hidden unit tests are going to evaluate if all desired properties of the required function are met. \n",
    "- If the function passes a test all the associated marks are awarded, if it fails 0 marks are awarded.\n",
    "- If you make a typo error (e.g. misspelling a variable) this will likely cause a syntax error, the function execution will fail and you will be awarded 0 marks.\n",
    "- Do not make assumptions on the state of previous cells, i.e. expect each function to be evaluated independently, moreover expect each function to be tested in the unit tests on some *randomly* generated input.\n",
    "\n",
    "Although the test uses a hard fail/pass strategy to assign marks, the presence of several questions allows a fine grading. \n",
    "\n",
    "The Checkpoints are not graded by default but might be used to assign additional marks in case the execution of the code obtains the desired results even when some tests might fail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b88ba4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d50809ffb1544ab086cbd6ecdef8d766",
     "grade": false,
     "grade_id": "cell-ddb29af9845c75cc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Conventions and notation:\n",
    "\n",
    "Do not assume any library is avaialble other than `matplotlib`, `numpy`, `scipy`, `pandas` and `sklearn`.\n",
    "\n",
    "Assume Python 3.8.\n",
    "\n",
    "---\n",
    "\n",
    "In the rest of the notebook, the term `DataFrame` refers to a Pandas DataFrame object. \n",
    "\n",
    "In the rest of the notebook, the term `data matrix` refers to a two dimensional numpy array where instances are encoded as rows, e.g. a data matrix with 100 rows and 4 columns is to be interpreted as a collection of 100 instances (vectors) each of dimension four.\n",
    "\n",
    "In the rest of the notebook, the term `vector` refers to a one dimensional numpy array. \n",
    "\n",
    "When we explicitly use the term `column vector` we mean a 2 dimensional array of shape `(n,1)`, when we explicitly use the term `row vector` we mean a 2 dimensional vector of shape `(1,n)`.\n",
    "\n",
    "When the term `distance` is used we mean the Euclidean distance. \n",
    "\n",
    "The functions you are required to write often need to take in input and return as output such objects, not python lists. Check the specifications for each required function. \n",
    "\n",
    "---\n",
    "\n",
    "**Do not use library functions** to directly solve a question unless explicity instructed to do so. That is, when a required function can be implemented directly by a library function it is intended that the candidate should write their own implementation of the function: for example it the Question asks to implement a function to compute the `accuracy` one cannot just wrap the function `accuracy_score` from `sklearn.metrics` in a custom function.\n",
    "\n",
    "---\n",
    "\n",
    "Do not assume that the implementations provided in the Workshops exercises contain no mistakes. You should write and are ultimately responsible for the code that you submit in this Assessment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2738bb9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T16:49:42.456261Z",
     "start_time": "2023-01-12T16:49:28.389469Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7d8e5fb5b3358a27507be026f78869df",
     "grade": false,
     "grade_id": "cell-864afe73677aa10c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from submission_utils import save_history, check_and_prepare_for_submission\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed8ec0e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5e4260564da813b97d3b29de3eba200f",
     "grade": false,
     "grade_id": "cell-59810fa699f9cbd0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 1\n",
    "\n",
    "\n",
    "### Function: `create_dataset`\n",
    "<div style=\"text-align: right\"><b>[20 marks]</b></div>\n",
    "\n",
    "The `create_dataset` function generates synthetic data for multiple clusters in an arbitrary dimensional space. Cluster centers are sampled from a normal distribution with a mean of zero and a user-defined standard deviation. Each cluster's center, number of points, and spread are defined by the function parameters, making it possible to create a dataset with clusters of varying densities in high-dimensional space.\n",
    "\n",
    "#### Function Signature:\n",
    "```python\n",
    "\n",
    "\n",
    "def create_dataset(num_clusters: int, \n",
    "                   num_dimensions: int, \n",
    "                   num_points: np.ndarray, \n",
    "                   cluster_std_devs: np.ndarray, \n",
    "                   center_std_dev: float) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generates synthetic data for multiple clusters with random centers in arbitrary dimensional space.\n",
    "\n",
    "    Parameters:\n",
    "    - num_clusters (int): Number of clusters to generate.\n",
    "    - num_dimensions (int): Dimensionality of the space (e.g., 3 for 3D, etc.).\n",
    "    - num_points (np.ndarray): Array of shape (num_clusters,) specifying the number of points in each cluster.\n",
    "    - cluster_std_devs (np.ndarray): Array of shape (num_clusters,), specifying the standard deviation for each cluster's spread.\n",
    "    - center_std_dev (float): Standard deviation for generating cluster centers from a normal distribution with zero mean.\n",
    "\n",
    "    Returns:\n",
    "    - data (np.ndarray): A 2D array of shape (total_points, num_dimensions), with points generated in d-dimensional space.\n",
    "    - labels (np.ndarray): A 1D array of shape (total_points,), containing cluster labels for each point.\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c0484d80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T16:49:42.469254Z",
     "start_time": "2023-01-12T16:49:42.461212Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e4f9ee511814c5cf7530730813faa187",
     "grade": false,
     "grade_id": "cell-fb96f63eef3fae06",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def create_dataset(num_clusters: int, \n",
    "                   num_dimensions: int, \n",
    "                   num_points: np.ndarray, \n",
    "                   cluster_std_devs: np.ndarray, \n",
    "                   center_std_dev: float) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    data = []\n",
    "    labels = []\n",
    "    origin = np.zeros((num_dimensions,))\n",
    "    cluster_centres = np.random.normal(loc=origin, scale=center_std_dev, size=(num_clusters, num_dimensions))\n",
    "    for i in range(num_clusters):\n",
    "        cluster_points = np.random.normal(loc=cluster_centres[i], scale=cluster_std_devs[i], size=(num_points[i], num_dimensions))\n",
    "        for point in cluster_points:\n",
    "            data.append(point)\n",
    "            labels.append(i)\n",
    "    return (np.array(data), np.array(labels))\n",
    "# data, labels = create_dataset(4, 5, np.array([4, 4, 4, 4, 4]), np.array([1, 3, 5, 7, 9]), 10)\n",
    "# print(data)\n",
    "save_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff477cf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T16:49:42.492152Z",
     "start_time": "2023-01-12T16:49:42.475045Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "97443629832fb9dc74c0fa9f25c24eaa",
     "grade": true,
     "grade_id": "cell-b20523e7b3a26572",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172537e7-eea3-4003-b4bf-bb84313a65e7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c8bb7d4fd2376549a5467513ee2146fe",
     "grade": true,
     "grade_id": "cell-d558243f71d9fe8d",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2494a73f-6542-43a3-b888-42866a514e8e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f4704f408b9bc5bb5191f789f1829ba5",
     "grade": false,
     "grade_id": "cell-e21336f7b62a005c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 2\n",
    "\n",
    "### Implement PCA Using SVD\n",
    "<div style=\"text-align: right\"><b>[7 marks]</b></div>\n",
    "\n",
    "Implement a function that performs Principal Component Analysis (PCA) on a dataset by using Singular Value Decomposition (SVD). The function should first center the data matrix by subtracting the mean from each feature, then apply SVD to obtain the principal components. The function should return the data projected onto the specified number of principal components.\n",
    "\n",
    "#### Function Signature:\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def pca_with_svd(data: np.ndarray, n_components: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Performs PCA on the given data using Singular Value Decomposition (SVD).\n",
    "    First, it centers the data matrix, then applies SVD to extract the principal components.\n",
    "\n",
    "    Parameters:\n",
    "    - data (np.ndarray): A 2D array of shape (n_samples, n_features), representing the input data matrix.\n",
    "    - n_components (int): The number of principal components to retain.\n",
    "\n",
    "    Returns:\n",
    "    - transformed_data (np.ndarray): A 2D array of shape (n_samples, n_components), \n",
    "                                     the data projected onto the top principal components.\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a4409e-8a9f-4e3b-8daa-7225a720cac4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "97c3c5b9cf0ba55b00e80e0cf91e530f",
     "grade": false,
     "grade_id": "cell-1e93111866e40c02",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def pca_with_svd(data: np.ndarray, n_components: int) -> np.ndarray:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "save_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9422358-a1ee-49ec-9543-e35ff0d489f1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2407c509dfd1c1fed6ed1b4adf66a870",
     "grade": true,
     "grade_id": "cell-7c20b0adf57e1638",
     "locked": true,
     "points": 7,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8e34f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-28T15:49:34.862891Z",
     "start_time": "2022-10-28T15:49:34.844291Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "66332d83f304aa1ef6c781024aad700f",
     "grade": false,
     "grade_id": "cell-91370583a840187d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 3\n",
    "\n",
    "\n",
    "### Implement `plot_dataset`\n",
    "<div style=\"text-align: right\"><b>[3 marks]</b></div>\n",
    "\n",
    "The `plot_dataset` function visualizes the dataset generated by `create_dataset` by projecting high-dimensional data onto a 2D plane using Principal Component Analysis (PCA) implemented in `pca_with_svd`. This projection enables visualization of clusters in an interpretable 2D space, regardless of the original dimensionality of the data.\n",
    "\n",
    "#### Function Signature:\n",
    "```python\n",
    "def plot_dataset(data: np.ndarray, labels: np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    Projects high-dimensional data to 2D using PCA and plots the dataset with color-coding for each cluster.\n",
    "\n",
    "    Parameters:\n",
    "    - data (np.ndarray): A 2D array of shape (total_points, num_dimensions), containing points in d-dimensional space.\n",
    "    - labels (np.ndarray): A 1D array of shape (total_points,), containing cluster labels for each point.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a689a184",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T16:49:42.506565Z",
     "start_time": "2023-01-12T16:49:42.499919Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a1dc267eaaa2318ab7c57eab3227b858",
     "grade": false,
     "grade_id": "cell-b527fe7368f3026c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def plot_dataset(data: np.ndarray, labels: np.ndarray) -> None:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "save_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd72e06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T16:49:42.525848Z",
     "start_time": "2023-01-12T16:49:42.511250Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ff0fe2c1bcc870d0b65f3da66a366824",
     "grade": true,
     "grade_id": "cell-6a42c2b5c66addcd",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515a4cdc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "53f24463f1afc66ed022660955f5a84b",
     "grade": false,
     "grade_id": "cell-aba1151d8e09cb69",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 4\n",
    "\n",
    "### Implement a Function to Impute Missing Values Using Low-Rank SVD\n",
    "<div style=\"text-align: right\"><b>[20 marks]</b></div>\n",
    "\n",
    "Implement a function `impute_missing_values` that takes a data matrix (as a NumPy array) and a rank parameter as inputs. This function should replace all `NaN` values in the data matrix with values approximated using a low-rank SVD technique. Here’s how you should proceed:\n",
    "\n",
    "1. **Replace NaN with Zeros**:  \n",
    "   Start by replacing all `NaN` values in the data matrix with zeros. This will allow the matrix to be processed by the SVD algorithm, which does not handle missing values.\n",
    "\n",
    "2. **Compute Low-Rank SVD**:  \n",
    "   Apply Singular Value Decomposition (SVD) to the modified matrix. Use the specified `rank` parameter to reconstruct an approximation of the matrix that only retains the top `rank` singular values and their associated singular vectors.\n",
    "\n",
    "3. **Replace Only Original NaN Entries**:  \n",
    "   Use the low-rank SVD approximation to fill in only the entries that were originally `NaN` in the data matrix. This will replace the missing values with approximations derived from the low-rank reconstruction, while preserving other entries.\n",
    "\n",
    "---\n",
    "\n",
    "### Function Signature\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "def impute_missing_values(data: np.ndarray, rank: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Imputes missing values in a data matrix using low-rank SVD approximation.\n",
    "\n",
    "    Parameters:\n",
    "    - data (np.ndarray): A 2D array of shape (n_samples, n_features) containing the input data matrix, \n",
    "                         with NaN values representing missing entries.\n",
    "    - rank (int): The rank for the low-rank SVD approximation.\n",
    "\n",
    "    Returns:\n",
    "    - imputed_data (np.ndarray): A 2D array of the same shape as `data`, with NaN values replaced by approximations.\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "The function should return `imputed_data`, a 2D NumPy array of the same shape as the input `data`, with all `NaN` values replaced by approximated values based on the low-rank SVD reconstruction.\n",
    "\n",
    "This approach will effectively use low-rank SVD to fill missing values in a way that approximates the overall structure of the data, leveraging the patterns present in the available entries.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52773e42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T16:49:42.538669Z",
     "start_time": "2023-01-12T16:49:42.530600Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "290d7b0c9d357bb18ac25ca22b2851fa",
     "grade": false,
     "grade_id": "cell-db112409212829b2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def impute_missing_values(data: np.ndarray, rank: int) -> np.ndarray:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "save_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762c28b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-12T17:02:25.529769Z",
     "start_time": "2023-01-12T17:02:25.519336Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7b79fdf91359a8e238fc5f57e1f3b390",
     "grade": true,
     "grade_id": "cell-b724ca85a4180e93",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f4ba48-6913-4898-a6ae-3a2b2eeb2f81",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "62c35d95ca7982a8488d62435c3579b5",
     "grade": false,
     "grade_id": "cell-6db58d0dca3413a1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Checkpoint\n",
    "\n",
    "This is just a check-point, i.e. it is for you to see that you are correctly implementing all functions.\n",
    "\n",
    "By executing the following code (just copy the code the next cell):\n",
    "\n",
    "1. **Create a 5-dimensional dataset** with clusters.\n",
    "2. **Plot the original data** (using PCA for 2D projection).\n",
    "3. **Introduce 10% missing values** by randomly setting entries to `NaN`.\n",
    "4. **Use `impute_missing_values`** to fill in missing values with a rank-4 approximation.\n",
    "5. **Plot the imputed data** to visualize the differences after imputation.\n",
    "\n",
    "```python\n",
    "\n",
    "# Step 1: Create a 5-dimensional dataset with 3 clusters\n",
    "num_clusters = 3\n",
    "num_dimensions = 5\n",
    "num_points = np.array([100, 100, 100])\n",
    "cluster_std_devs = np.array([1.0, 1.0, 1.0])\n",
    "center_std_dev = 3.0\n",
    "\n",
    "data, labels = create_dataset(num_clusters, num_dimensions, num_points, cluster_std_devs, center_std_dev)\n",
    "\n",
    "# Step 2: Plot the original data (2D projection using PCA)\n",
    "print(\"Original Data (2D projection)\")\n",
    "plot_dataset(data, labels)\n",
    "\n",
    "# Step 3: Introduce 10% missing values\n",
    "nan_data = data.copy()\n",
    "num_elements = nan_data.size\n",
    "num_nan = int(0.1 * num_elements)  # 10% of the data\n",
    "\n",
    "# Randomly choose 10% of indices to set as NaN\n",
    "nan_indices = np.random.choice(num_elements, num_nan, replace=False)\n",
    "nan_data.ravel()[nan_indices] = np.nan\n",
    "\n",
    "# Step 4: Impute missing values using rank-4 SVD approximation\n",
    "imputed_data = impute_missing_values(nan_data, rank=4)\n",
    "\n",
    "# Step 5: Plot the imputed data (2D projection using PCA)\n",
    "print(\"Imputed Data (2D projection)\")\n",
    "plot_dataset(imputed_data, labels)\n",
    "```\n",
    "\n",
    "you should obtain an output similar to:\n",
    "\n",
    "<img src=\"plot_a.png\" width=60%>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf51e364-755b-46b2-bfc4-d0a5f79e8e29",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "733b22e3f4c07e53b4c93b5e7f8baa40",
     "grade": false,
     "grade_id": "cell-be2e05a561ee8e3d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Create a 5-dimensional dataset with 3 clusters\n",
    "num_clusters = 3\n",
    "num_dimensions = 5\n",
    "num_points = np.array([100, 100, 100])\n",
    "cluster_std_devs = np.array([1.0, 1.0, 1.0])\n",
    "center_std_dev = 3.0\n",
    "\n",
    "data, labels = create_dataset(num_clusters, num_dimensions, num_points, cluster_std_devs, center_std_dev)\n",
    "\n",
    "# Step 2: Plot the original data (2D projection using PCA)\n",
    "print(\"Original Data (2D projection)\")\n",
    "plot_dataset(data, labels)\n",
    "\n",
    "# Step 3: Introduce 10% missing values\n",
    "nan_data = data.copy()\n",
    "num_elements = nan_data.size\n",
    "num_nan = int(0.1 * num_elements)  # 10% of the data\n",
    "\n",
    "# Randomly choose 10% of indices to set as NaN\n",
    "nan_indices = np.random.choice(num_elements, num_nan, replace=False)\n",
    "nan_data.ravel()[nan_indices] = np.nan\n",
    "\n",
    "# Step 4: Impute missing values using rank-4 SVD approximation\n",
    "imputed_data = impute_missing_values(nan_data, rank=4)\n",
    "\n",
    "# Step 5: Plot the imputed data (2D projection using PCA)\n",
    "print(\"Imputed Data (2D projection)\")\n",
    "plot_dataset(imputed_data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bb96c6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4588236bd05fbc7e514b3ca36589a94d",
     "grade": false,
     "grade_id": "cell-a9f7ee99c0d968be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 5\n",
    "\n",
    "## Implement a basic version of the ISOMAP algorithm\n",
    "\n",
    "### Summary of the ISOMAP Workflow\n",
    "\n",
    "1. **Construct k-NN Graph**: Create a graph that connects each point to its k nearest neighbors and stores edge weights as distances.\n",
    "2. **Compute Geodesic Distances**: Calculate the shortest paths between all pairs of points in the k-NN graph to estimate the geodesic distances.\n",
    "3. **Apply MDS**: Use MDS on the geodesic distance matrix to reduce the dataset to a lower-dimensional space, preserving the manifold structure.\n",
    "\n",
    "---\n",
    "\n",
    "### Sub-problem 1: Construct a k-Nearest Neighbor Graph\n",
    "<div style=\"text-align: right\"><b>[6 marks]</b></div>\n",
    "\n",
    "The first step in implementing ISOMAP is to create a k-nearest neighbor (k-NN) graph from the dataset. For each point in the dataset, find its `k` nearest neighbors and connect them with edges. Store the distances between these neighbors as edge weights in an adjacency matrix. The resulting k-NN graph will approximate the local relationships in the data, which will be used to estimate the geodesic distances in the next step. \n",
    "\n",
    "**Note**: For this sub-problem, you are *not required to implement your own version of k-nearest neighbor*. You may use the NearestNeighbors implementation provided by scikit-learn, which can be found here: [NearestNeighbors](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html)\n",
    "\n",
    "#### Function Signature\n",
    "```python\n",
    "def construct_knn_graph(data: np.ndarray, k: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Constructs a k-nearest neighbor (k-NN) graph from a dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - data (np.ndarray): A 2D array of shape (n_samples, n_features), representing the dataset.\n",
    "    - k (int): The number of nearest neighbors to connect for each point.\n",
    "\n",
    "    Returns:\n",
    "    - knn_graph (np.ndarray): A 2D array of shape (n_samples, n_samples), where knn_graph[i, j] \n",
    "                              contains the distance between points i and j if they are neighbors,\n",
    "                              or 0 otherwise.\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Sub-problem 2: Compute the Shortest Path Distances\n",
    "<div style=\"text-align: right\"><b>[6 marks]</b></div>\n",
    "\n",
    "In this step, use the k-NN graph from the previous step to calculate the shortest path distances between all pairs of points. These shortest path distances approximate the geodesic distances, which measure the true distance along the manifold. You can use the `dijkstra` function from `scipy.sparse.csgraph` to compute the shortest paths in the k-NN graph.\n",
    "\n",
    "**Note**: For this sub-problem, you are *not required to implement your own version of shortest path distances*. You may use the dijkstra implementation provided by scipy, which can be found here: [dijkstra](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csgraph.dijkstra.html)\n",
    "\n",
    "\n",
    "#### Function Signature\n",
    "```python\n",
    "def compute_geodesic_distances(knn_graph: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the shortest path distances (geodesic distances) between all pairs of points \n",
    "    in the k-nearest neighbor graph using the Floyd-Warshall or Dijkstra's algorithm.\n",
    "\n",
    "    Parameters:\n",
    "    - knn_graph (np.ndarray): A 2D array of shape (n_samples, n_samples), representing the k-NN graph \n",
    "                              where knn_graph[i, j] contains the distance between points i and j if they \n",
    "                              are neighbors, or 0 otherwise.\n",
    "\n",
    "    Returns:\n",
    "    - geodesic_distances (np.ndarray): A 2D array of shape (n_samples, n_samples) containing \n",
    "                                       the shortest path (geodesic) distance between all pairs of points.\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Sub-problem 3: Apply Multidimensional Scaling (MDS)\n",
    "<div style=\"text-align: right\"><b>[6 marks]</b></div>\n",
    "\n",
    "Once we have the geodesic distance matrix from Sub-problem 2, the next step is to apply Multidimensional Scaling (MDS) to reduce the data to a lower-dimensional space. MDS will attempt to preserve the pairwise distances in the reduced space, closely matching them to the original geodesic distances. This projection effectively retains the manifold’s structure, providing a meaningful lower-dimensional embedding.\n",
    "\n",
    "**Note**: For this sub-problem, you are *not required to implement your own version of MDS*. You may use the MDS implementation provided by scikit-learn, which can be found here: [sklearn.manifold.MDS](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html). \n",
    "\n",
    "#### Function Signature\n",
    "```python\n",
    "def apply_mds(geodesic_distances: np.ndarray, n_components: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Applies Multidimensional Scaling (MDS) to the geodesic distance matrix to reduce the data \n",
    "    to a lower-dimensional space.\n",
    "\n",
    "    Parameters:\n",
    "    - geodesic_distances (np.ndarray): A 2D array of shape (n_samples, n_samples), containing \n",
    "                                       the geodesic distances between all pairs of points.\n",
    "    - n_components (int): The number of dimensions to project the data onto (e.g., 2 for 2D).\n",
    "\n",
    "    Returns:\n",
    "    - reduced_data (np.ndarray): A 2D array of shape (n_samples, n_components), representing \n",
    "                                 the data in the reduced-dimensional space.\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Sub-problem 4: ISOMAP\n",
    "<div style=\"text-align: right\"><b>[6 marks]</b></div>\n",
    "\n",
    "The function signature for the main `ISOMAP` function should take in a dataset, parameters for the algorithm, and return the data projected onto a lower-dimensional space. It will use the helper functions to construct the k-nearest neighbor graph, compute geodesic distances, and apply Multidimensional Scaling (MDS) to the geodesic distance matrix.\n",
    "\n",
    "```python\n",
    "def isomap(data: np.ndarray, n_components: int, k: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Applies the ISOMAP algorithm to reduce high-dimensional data to a lower-dimensional space,\n",
    "    preserving the manifold structure of the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - data (np.ndarray): A 2D array of shape (n_samples, n_features), representing the dataset.\n",
    "    - n_components (int): The number of dimensions to project the data onto (e.g., 2 for 2D).\n",
    "    - k (int): The number of nearest neighbors to connect for each point in the k-NN graph.\n",
    "\n",
    "    Returns:\n",
    "    - reduced_data (np.ndarray): A 2D array of shape (n_samples, n_components), representing \n",
    "                                 the dataset in the reduced-dimensional space.\n",
    "    \"\"\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c577e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T09:56:55.070257Z",
     "start_time": "2022-12-02T09:56:55.066682Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8cd2070add04ed5e02662913a3f59cca",
     "grade": false,
     "grade_id": "cell-9508c7dc71ec2902",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def construct_knn_graph(data: np.ndarray, k: int) -> np.ndarray:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "save_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1486e73e-099c-4748-aa4e-c8030dedfbaa",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "760cc09f3b1e1e3e6289331d6eef6001",
     "grade": false,
     "grade_id": "cell-5f1b9639ec956d99",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from scipy.sparse.csgraph import dijkstra\n",
    "\n",
    "def compute_geodesic_distances(knn_graph: np.ndarray) -> np.ndarray:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "save_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c96fe7-76cd-4dfa-b060-4d0218f38564",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "46f0d300423bfc1d48f769fc0be4cb17",
     "grade": false,
     "grade_id": "cell-76786e650a54843d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "\n",
    "def apply_mds(geodesic_distances: np.ndarray, n_components: int) -> np.ndarray:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "save_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbca3797-1a77-4e28-836d-04371b0fb7e6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "063efe1c7e39b1b2da977c7be3f45bb5",
     "grade": false,
     "grade_id": "cell-ec33c2e26d605b6d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def isomap(data: np.ndarray, n_components: int, k: int) -> np.ndarray:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "save_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6441f08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T10:02:27.078897Z",
     "start_time": "2022-12-02T10:02:27.067233Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4dc89445b1595ebbbd75680ab3ae2796",
     "grade": true,
     "grade_id": "cell-129d8595962ee97b",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3f233f-8611-40a4-bda4-ffb4da5f69da",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2884e470aba380f6e87fc899a8a25114",
     "grade": true,
     "grade_id": "cell-4a55edcb0b77a7c6",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd5cb30-acb6-4894-9fab-9eb717b2c9af",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5d4ef58289d9ff16b1bd2bbb61dec448",
     "grade": true,
     "grade_id": "cell-d3b12edc98b84fcd",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35d2fdc-2966-4b3d-9fc0-27af09462663",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ec0231e9bceaa9225950091a52bead03",
     "grade": true,
     "grade_id": "cell-fc467f34fe7431f0",
     "locked": true,
     "points": 6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1fe434-3a23-4b71-88d2-e64592726a9b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4c53984b1fd33c4ac6853262aae614d9",
     "grade": false,
     "grade_id": "cell-f58fac2652aec086",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 6\n",
    "\n",
    "### Implement K-Means Clustering Algorithm\n",
    "\n",
    "Implement the K-Means algorithm from scratch, where you iteratively assign data points to clusters based on proximity to centroids and update the centroids. This exercise will provide a solid understanding of how K-Means works internally.\n",
    "\n",
    "\n",
    "1. **Initialize Centroids**:  \n",
    "   Randomly initialize `k` centroids by selecting `k` data points as the starting cluster centers.\n",
    "\n",
    "2. **Assign Points to Nearest Centroid**:  \n",
    "   For each point in the dataset, assign it to the nearest centroid based on Euclidean distance.\n",
    "\n",
    "3. **Update Centroids**:  \n",
    "   For each cluster, calculate the new centroid as the mean of all points assigned to that cluster.\n",
    "\n",
    "4. **Iterate**:  \n",
    "   Repeat steps 2 and 3 until convergence (i.e., the centroids no longer change significantly or a maximum number of iterations is reached).\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Initialize Centroids\n",
    "<div style=\"text-align: right\"><b>[3 marks]</b></div>\n",
    "\n",
    "This function selects `k` initial centroids by randomly choosing `k` unique data points from the dataset. This helps start the K-Means algorithm with an initial set of cluster centers. Random initialization can impact clustering results, so this step can be run multiple times if necessary for better stability.\n",
    "\n",
    "#### Function Signature\n",
    "\n",
    "```python\n",
    "def initialize_centroids(data: np.ndarray, k: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Randomly selects `k` unique data points from the dataset as initial centroids.\n",
    "\n",
    "    Parameters:\n",
    "    - data (np.ndarray): A 2D array of shape (n_samples, n_features), representing the dataset.\n",
    "    - k (int): The number of clusters (and thus, the number of centroids) to initialize.\n",
    "\n",
    "    Returns:\n",
    "    - centroids (np.ndarray): A 2D array of shape (k, n_features), representing the initial centroids.\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Assign Points to Nearest Centroid\n",
    "<div style=\"text-align: right\"><b>[3 marks]</b></div>\n",
    "\n",
    "This function assigns each data point to the nearest centroid based on Euclidean distance. For each point, it calculates the distance to each centroid and assigns the point to the cluster with the nearest centroid. This step defines the clustering structure for each iteration.\n",
    "\n",
    "#### Function Signature\n",
    "\n",
    "```python\n",
    "def assign_clusters(data: np.ndarray, centroids: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Assigns each data point to the nearest centroid based on Euclidean distance.\n",
    "\n",
    "    Parameters:\n",
    "    - data (np.ndarray): A 2D array of shape (n_samples, n_features), representing the dataset.\n",
    "    - centroids (np.ndarray): A 2D array of shape (k, n_features), representing the current centroids.\n",
    "\n",
    "    Returns:\n",
    "    - labels (np.ndarray): A 1D array of shape (n_samples,), where each entry is the index of the nearest centroid for each data point.\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Update Centroids\n",
    "<div style=\"text-align: right\"><b>[3 marks]</b></div>\n",
    "\n",
    "This function updates the centroid of each cluster by calculating the mean of all data points assigned to that cluster. For each centroid, it finds the mean position of all points within its cluster and updates the centroid’s position accordingly. This step moves the centroids closer to the “center” of their assigned points, reducing intra-cluster distances.\n",
    "\n",
    "#### Function Signature\n",
    "\n",
    "```python\n",
    "def update_centroids(data: np.ndarray, labels: np.ndarray, k: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Updates the centroid of each cluster by calculating the mean of points assigned to each cluster.\n",
    "\n",
    "    Parameters:\n",
    "    - data (np.ndarray): A 2D array of shape (n_samples, n_features), representing the dataset.\n",
    "    - labels (np.ndarray): A 1D array of shape (n_samples,), where each entry is the index of the assigned cluster for each point.\n",
    "    - k (int): The number of clusters.\n",
    "\n",
    "    Returns:\n",
    "    - new_centroids (np.ndarray): A 2D array of shape (k, n_features), representing the updated centroids.\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "### Step 4: K-Means algorithm\n",
    "<div style=\"text-align: right\"><b>[3 marks]</b></div>\n",
    "Implement the K-Means algorithm using the previous functions.\n",
    "\n",
    "The parameter `tol` in the K-Means algorithm represents the tolerance for convergence, defining the maximum allowable movement of centroids between iterations for the algorithm to be considered as having converged. When the maximum shift of any centroid is less than `tol`, the algorithm stops, as further adjustments are unlikely to significantly improve clustering accuracy.\n",
    "\n",
    "The parameter `max_iter` in the K-Means algorithm specifies the maximum number of iterations the algorithm will run before stopping, regardless of whether convergence has been achieved. It acts as a safeguard to prevent the algorithm from running indefinitely, especially in cases where the centroids oscillate or convergence is slow. If the algorithm reaches `max_iter` iterations without meeting the convergence criteria (based on `tol`), it stops, returning the current centroids and cluster assignments. This ensures computational efficiency by limiting the algorithm’s runtime.\n",
    "\n",
    "#### Function Signature\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def kmeans(data: np.ndarray, k: int, max_iter: int = 100, tol: float = 1e-4) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Implements the K-Means clustering algorithm from scratch.\n",
    "\n",
    "    Parameters:\n",
    "    - data (np.ndarray): A 2D array of shape (n_samples, n_features) representing the dataset.\n",
    "    - k (int): The number of clusters to form.\n",
    "    - max_iter (int): The maximum number of iterations for convergence.\n",
    "    - tol (float): The tolerance for convergence based on centroid movement.\n",
    "\n",
    "    Returns:\n",
    "    - centroids (np.ndarray): A 2D array of shape (k, n_features) representing the final centroid positions.\n",
    "    - labels (np.ndarray): A 1D array of shape (n_samples,) where each entry is the assigned cluster label for each point.\n",
    "    \"\"\"\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ae0636-ecc7-45e9-80f1-d8d1ecac2de0",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e696053a2995c6809dc1049c5f4a2541",
     "grade": false,
     "grade_id": "cell-6f74830ab3e7571f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def initialize_centroids(data: np.ndarray, k: int) -> np.ndarray:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "save_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83484ad-6375-4958-aaa1-15e95d1f4825",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3e577124574f8d1b496be1cabfa4b9c5",
     "grade": true,
     "grade_id": "cell-c024d5edc41f4143",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a291dd1-af8c-4074-938e-0955a22181f9",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "14fe60cca2454c5f7ef23c6fb3777acd",
     "grade": false,
     "grade_id": "cell-afa4695f281ce373",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def assign_clusters(data: np.ndarray, centroids: np.ndarray) -> np.ndarray:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "save_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc933a0-626e-40f2-9aea-6e18b890234c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "862de34092c29923403fab77f76cd463",
     "grade": true,
     "grade_id": "cell-9487188fa18229f5",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f085d27-d158-423b-b076-2d018cbaf405",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ac7080d9c9347d0cd1706be9f3daa4d",
     "grade": false,
     "grade_id": "cell-c2a163459544f341",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def update_centroids(data: np.ndarray, labels: np.ndarray, k: int) -> np.ndarray:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "save_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e972197a-5d10-4a22-86c7-71365b0a4d9e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c43e9d661bc8190c2ef1b9b3c2f938d2",
     "grade": true,
     "grade_id": "cell-104c283dc1514d39",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef484bf-645a-4473-bc31-9796c33707fe",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "27997156d43d010aa64650c07e3c3fa7",
     "grade": false,
     "grade_id": "cell-d240f55da6c9ca25",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def kmeans(data: np.ndarray, k: int, max_iter: int = 100, tol: float = 1e-4) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "save_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3306f79b-bcfe-473a-912e-d9534beceeef",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b39c7daa9d6048427e55d30ee9bd03fa",
     "grade": true,
     "grade_id": "cell-957685c1b4a79ced",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b3c078",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fc5b3f6e16b21723488f18cb8a2b50c1",
     "grade": false,
     "grade_id": "cell-82e09d4297b5576a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 7\n",
    "\n",
    "\n",
    "### Implement the Silhouette Score \n",
    "The silhouette score is a measure of how well data points are clustered. For each data point $ i $, the silhouette score $ s(i) $ is calculated based on two main values: the **within-cluster distance** $ a(i) $ and the **nearest-cluster distance** $ b(i) $. The silhouette score helps evaluate the quality of clustering results, with higher scores indicating better-defined clusters.\n",
    "\n",
    "\n",
    "### Summary of Functions\n",
    "\n",
    "1. **`within_cluster_distances`**: Computes $ a(i) $ for each point.\n",
    "2. **`nearest_cluster_distances`**: Computes $ b(i) $ for each point.\n",
    "3. **`silhouette_score`**: Uses $ a(i) $ and $ b(i) $ to compute the silhouette score $ s(i) $ for each point and the mean silhouette score for the dataset.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### Sub-problem 1: Compute the Within-Cluster Distance $a(i)$\n",
    "<div style=\"text-align: right\"><b>[5 marks]</b></div>\n",
    "\n",
    "For each data point $ i $, calculate $ a(i) $, the **within-cluster distance**. This is the average distance from $ i $ to all other points within the same cluster. It provides a measure of how similar each point is to other points in its own cluster.\n",
    "\n",
    "#### Function Signature\n",
    "```python\n",
    "\n",
    "def within_cluster_distances(data: np.ndarray, labels: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the within-cluster distance a(i) for each data point, representing the average distance\n",
    "    from each point to all other points in the same cluster.\n",
    "\n",
    "    Parameters:\n",
    "    - data (np.ndarray): A 2D array of shape (n_samples, n_features), representing the dataset.\n",
    "    - labels (np.ndarray): A 1D array of shape (n_samples,), where each entry indicates the cluster label for each point.\n",
    "\n",
    "    Returns:\n",
    "    - within_distances (np.ndarray): A 1D array of shape (n_samples,), where each entry is the within-cluster distance a(i) for each point.\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Sub-problem 2: Compute the Nearest-Cluster Distance \\( b(i) \\)\n",
    "<div style=\"text-align: right\"><b>[5 marks]</b></div>\n",
    "\n",
    "For each data point $ i $, calculate $ b(i) $, the **nearest-cluster distance**. This is defined as the minimum average distance from $ i $ to all points in any other cluster. The nearest-cluster distance indicates how close each point is to the points in the neighboring cluster.\n",
    "\n",
    "#### Function Signature\n",
    "```python\n",
    "def nearest_cluster_distances(data: np.ndarray, labels: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the nearest-cluster distance b(i) for each data point, which is the minimum average distance\n",
    "    from each point to all points in any other cluster.\n",
    "\n",
    "    Parameters:\n",
    "    - data (np.ndarray): A 2D array of shape (n_samples, n_features), representing the dataset.\n",
    "    - labels (np.ndarray): A 1D array of shape (n_samples,), where each entry indicates the cluster label for each point.\n",
    "\n",
    "    Returns:\n",
    "    - nearest_distances (np.ndarray): A 1D array of shape (n_samples,), where each entry is the nearest-cluster distance b(i) for each point.\n",
    "    \"\"\"\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Sub-problem 3: Compute the Silhouette Score\n",
    "<div style=\"text-align: right\"><b>[4 marks]</b></div>\n",
    "\n",
    "Using $ a(i) $ and $ b(i) $, calculate the silhouette score $ s(i) $ for each point. The silhouette score $ s(i) $ is defined as:\n",
    "\n",
    "$$\n",
    "s(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))}\n",
    "$$\n",
    "\n",
    "This score ranges from -1 to 1, where a high positive value indicates that the point is well-matched to its own cluster, and a value near zero or negative suggests poor clustering. The function should return both individual scores $ s(i) $ and the average silhouette score for the entire dataset, which provides an overall quality measure for the clustering.\n",
    "\n",
    "#### Function Signature\n",
    "```python\n",
    "def silhouette_score(data: np.ndarray, labels: np.ndarray) -> Tuple[np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    Calculates the silhouette score s(i) for each data point and the mean silhouette score for the entire dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - data (np.ndarray): A 2D array of shape (n_samples, n_features), representing the dataset.\n",
    "    - labels (np.ndarray): A 1D array of shape (n_samples,), where each entry indicates the cluster label for each point.\n",
    "\n",
    "    Returns:\n",
    "    - scores (np.ndarray): A 1D array of shape (n_samples,), where each entry is the silhouette score s(i) for each point.\n",
    "    - mean_score (float): The mean silhouette score for the entire dataset.\n",
    "    \"\"\"\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2898b30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T10:03:27.258937Z",
     "start_time": "2022-12-02T10:03:27.254466Z"
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9e810a78e9132dd9ba55a6cb86eb88a0",
     "grade": false,
     "grade_id": "cell-6659e27b0e647ca0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def within_cluster_distances(data: np.ndarray, labels: np.ndarray) -> np.ndarray:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "save_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42ca6b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-02T10:04:21.918647Z",
     "start_time": "2022-12-02T10:04:21.915134Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b1af5ffa74fb18d64b304ecae5a66a63",
     "grade": true,
     "grade_id": "cell-6c9c1c2d835bb7ed",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbadc6fc-d6e2-4828-a19e-114d56d60200",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8aae2c040a3ed2f2fc979235057a9283",
     "grade": false,
     "grade_id": "cell-1fc730db2dc84cc9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def nearest_cluster_distances(data: np.ndarray, labels: np.ndarray) -> np.ndarray:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "save_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0aa6e9-bfdc-488b-b453-f7053c763e94",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a2d05a4c5229d744b68bd27004cd15b2",
     "grade": true,
     "grade_id": "cell-1e9f714c2bce2352",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9510cb56-206d-4c48-b3d7-8774f8f6889b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aa34a207dd8e8aaac9a70af1c50decb5",
     "grade": false,
     "grade_id": "cell-3af55560921e106a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def silhouette_score(data: np.ndarray, labels: np.ndarray) -> Tuple[np.ndarray, float]:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "save_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d47e17c-2a57-4822-8acc-0a5399144e9f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "acb68ee5b2b1a6e7dfd56c0f69e4a8f0",
     "grade": true,
     "grade_id": "cell-4b368bbe50d8fd9b",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is reserved for the unit tests. Do not consider this cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e69ff8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7f64651ca16e0d6f9a20bd70ce0ac824",
     "grade": false,
     "grade_id": "cell-85e9756eeb0755b5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Checkpoint\n",
    "\n",
    "This is just a check-point, i.e. it is for you to see that you are correctly implementing all functions.\n",
    "\n",
    "By executing the following code (just copy the code the next cell):\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Generate 5-dimensional data with overlapping clusters\n",
    "# We create 3 clusters with significant overlap\n",
    "np.random.seed(42)\n",
    "num_clusters = 3\n",
    "num_dimensions = 5\n",
    "num_points = np.array([100, 100, 100])\n",
    "cluster_std_devs = np.array([1.5, 2.5, 2.5])  # High std dev for overlap\n",
    "center_std_dev = 5.0  # Spread out the cluster centers more\n",
    "\n",
    "# Generate the dataset\n",
    "data, labels = create_dataset(num_clusters, num_dimensions, num_points, cluster_std_devs, center_std_dev)\n",
    "\n",
    "# Step 2: Introduce missing values into the dataset (10% of the data)\n",
    "nan_data = data.copy()\n",
    "num_elements = nan_data.size\n",
    "num_nan = int(0.1 * num_elements)  # 10% of the data\n",
    "nan_indices = np.random.choice(num_elements, num_nan, replace=False)\n",
    "nan_data.ravel()[nan_indices] = np.nan\n",
    "\n",
    "# Step 3: Impute missing values using low-rank SVD with rank 4\n",
    "imputed_data = impute_missing_values(nan_data, rank=4)\n",
    "\n",
    "# Step 4: Apply ISOMAP to reduce to 2D for visualization\n",
    "n_components = 2\n",
    "data_2d = isomap(imputed_data, n_components=n_components, k=5)\n",
    "\n",
    "# Step 5: Apply kmeans clustering and calculate silhouette scores for each cluster count\n",
    "n_clusters_range = range(2, 9)\n",
    "silhouette_scores = []\n",
    "\n",
    "for n_clusters in n_clusters_range:\n",
    "    # Apply kmeans clustering to the reduced data\n",
    "    centroids, cluster_labels = kmeans(data_2d, k=n_clusters, max_iter=100, tol=1e-4)\n",
    "    \n",
    "    # Calculate the silhouette score for the current number of clusters\n",
    "    _, mean_silhouette_score = silhouette_score(data_2d, cluster_labels)\n",
    "    silhouette_scores.append(mean_silhouette_score)\n",
    "\n",
    "# Step 6: Identify the best number of clusters based on silhouette score\n",
    "best_n_clusters = n_clusters_range[np.argmax(silhouette_scores)]\n",
    "best_silhouette_score = max(silhouette_scores)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Best number of clusters: {best_n_clusters}\")\n",
    "print(f\"Best silhouette score: {best_silhouette_score:.3f}\")\n",
    "\n",
    "# Plot silhouette scores vs. number of clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_clusters_range, silhouette_scores, marker='o')\n",
    "plt.title(\"Silhouette Scores for Different Numbers of Clusters\")\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Mean Silhouette Score\")\n",
    "plt.show()\n",
    "```\n",
    "you should obtain an output similar to:\n",
    "\n",
    "<img src=\"plot_b.png\" width=60%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd06837",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-31T17:32:56.286794Z",
     "start_time": "2022-10-31T17:32:55.465126Z"
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dfea126d73c0dd451a86437689dd2bed",
     "grade": false,
     "grade_id": "cell-8444e1ddc3d42bba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Generate 5-dimensional data with overlapping clusters\n",
    "# We create 3 clusters with significant overlap\n",
    "np.random.seed(42)\n",
    "num_clusters = 3\n",
    "num_dimensions = 5\n",
    "num_points = np.array([100, 100, 100])\n",
    "cluster_std_devs = np.array([1.5, 2.5, 2.5])  # High std dev for overlap\n",
    "center_std_dev = 5.0  # Spread out the cluster centers more\n",
    "\n",
    "# Generate the dataset\n",
    "data, labels = create_dataset(num_clusters, num_dimensions, num_points, cluster_std_devs, center_std_dev)\n",
    "\n",
    "# Step 2: Introduce missing values into the dataset (10% of the data)\n",
    "nan_data = data.copy()\n",
    "num_elements = nan_data.size\n",
    "num_nan = int(0.1 * num_elements)  # 10% of the data\n",
    "nan_indices = np.random.choice(num_elements, num_nan, replace=False)\n",
    "nan_data.ravel()[nan_indices] = np.nan\n",
    "\n",
    "# Step 3: Impute missing values using low-rank SVD with rank 4\n",
    "imputed_data = impute_missing_values(nan_data, rank=4)\n",
    "\n",
    "# Step 4: Apply ISOMAP to reduce to 2D for visualization\n",
    "n_components = 2\n",
    "data_2d = isomap(imputed_data, n_components=n_components, k=5)\n",
    "\n",
    "# Step 5: Apply kmeans clustering and calculate silhouette scores for each cluster count\n",
    "n_clusters_range = range(2, 9)\n",
    "silhouette_scores = []\n",
    "\n",
    "for n_clusters in n_clusters_range:\n",
    "    # Apply kmeans clustering to the reduced data\n",
    "    centroids, cluster_labels = kmeans(data_2d, k=n_clusters, max_iter=100, tol=1e-4)\n",
    "    \n",
    "    # Calculate the silhouette score for the current number of clusters\n",
    "    _, mean_silhouette_score = silhouette_score(data_2d, cluster_labels)\n",
    "    silhouette_scores.append(mean_silhouette_score)\n",
    "\n",
    "# Step 6: Identify the best number of clusters based on silhouette score\n",
    "best_n_clusters = n_clusters_range[np.argmax(silhouette_scores)]\n",
    "best_silhouette_score = max(silhouette_scores)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Best number of clusters: {best_n_clusters}\")\n",
    "print(f\"Best silhouette score: {best_silhouette_score:.3f}\")\n",
    "\n",
    "# Plot silhouette scores vs. number of clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_clusters_range, silhouette_scores, marker='o')\n",
    "plt.title(\"Silhouette Scores for Different Numbers of Clusters\")\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Mean Silhouette Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74887ccc-5188-4e45-bde3-f07e8d27af91",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4737b0d6f60142fda60ea5fee53ed53b",
     "grade": false,
     "grade_id": "cell-6d6977482a2e7e89",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Do not consider the next cell.\n",
    "# You do not have to do anything for the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5c8164-78b6-4a4f-828b-406728148a52",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85b661a23edcb6a522a0c6b1ed432809",
     "grade": true,
     "grade_id": "cell-2f108fbb04ee1ee6",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def8f65f-5268-48c0-bd14-aa12748545ef",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "90a546de37716f3cf389dd47b94256c0",
     "grade": false,
     "grade_id": "cell-8731e2a5f0c7fcad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "check_and_prepare_for_submission()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
