2024-11-17-14-12-16 104 1 get_ipython().run_line_magic('matplotlib', 'inline')	import matplotlib.pyplot as plt	from typing import Tuple	import numpy as np	import scipy as sp	from submission_utils import save_history, check_and_prepare_for_submission	# import warnings filter	from warnings import simplefilter	# ignore all future warnings	simplefilter(action='ignore', category=FutureWarning)
2024-11-17-14-12-16 104 2 def create_dataset(num_clusters: int, 	                   num_dimensions: int, 	                   num_points: np.ndarray, 	                   cluster_std_devs: np.ndarray, 	                   center_std_dev: float) -> Tuple[np.ndarray, np.ndarray]:	    data = []	    labels = []	    origin = np.zeros((num_dimensions,))	    cluster_centres = np.random.normal(loc=origin, scale=center_std_dev, size=(num_clusters, num_dimensions))	    for i in range(num_clusters):	        cluster_points = np.random.normal(loc=cluster_centres[i], scale=cluster_std_devs[i], size=(num_points[i], num_dimensions))	        for point in cluster_points:	            data.append(point)	            labels.append(i)	    data = np.array(data)	    labels = np.array(labels)	    return (data, labels)	# data, labels = create_dataset(4, 5, np.array([4, 4, 4, 4]), np.array([1, 3, 5, 7, 9]), 10)		save_history()
2024-11-17-14-12-16 104 3 # from sklearn.decomposition import PCA	# from sklearn.datasets import load_iris		def pca_with_svd(data: np.ndarray, n_components: int) -> np.ndarray:	    centred_data = data - np.mean(data, axis=0)	    u, s, vh = np.linalg.svd(centred_data)	    transformed_data = centred_data.dot(vh.T[:, :n_components])	    return transformed_data		# df = load_iris()	# X = df.data	# pca = PCA(n_components=2)	# X_proj = pca.fit_transform(X)	# print(X_proj)	# Y = df.data	# print(pca_with_svd(Y, 2))	save_history()
2024-11-17-14-12-16 104 4 import matplotlib.pyplot as plt	from sklearn.decomposition import PCA		def plot_dataset(data: np.ndarray, labels: np.ndarray) -> None:	    if (data.ndim == 1):	        return	    data_2d = PCA(n_components=2).fit_transform(data)	    plt.scatter(*data_2d.T, c=labels)	    plt.axis('equal')	    plt.xlabel('First PCA component')	    plt.ylabel('Second PCA component')	    plt.grid()	    plt.show()	    return		save_history()
2024-11-17-14-12-16 104 5 def impute_missing_values(data: np.ndarray, rank: int) -> np.ndarray:	    no_nan_data = np.nan_to_num(data, nan=0)	    u, s, vh = np.linalg.svd(no_nan_data)	    s = np.diag(s)	    num_cols_u = u.shape[1]	    if num_cols_u > rank:	        u_trunc = u[:, :rank]	    else:	        u_trunc = u	    num_cols_vh = vh.shape[1]	    if num_cols_vh > rank:	        vh_trunc = vh[:rank, :]	    else:	        vh_trunc = vh	    num_sinuglar_values = s.shape[0]	    if num_sinuglar_values > rank:	        s_trunc = s[:rank, :rank]	    else:	        s_trunc = s	    trunc_svd_data = np.dot(np.dot(u_trunc, s_trunc), vh_trunc)	    nan_mask = np.isnan(data)	    nan_indices = np.where(nan_mask)	    imputed_data = data.copy()	    for row_ind, col_ind in zip(nan_indices[0], nan_indices[1]):	        impute_value = trunc_svd_data[row_ind, col_ind]	        imputed_data[row_ind, col_ind] = impute_value	    return imputed_data		save_history()
2024-11-17-14-12-16 104 6 # Step 1: Create a 5-dimensional dataset with 3 clusters	num_clusters = 3	num_dimensions = 5	num_points = np.array([100, 100, 100])	cluster_std_devs = np.array([1.0, 1.0, 1.0])	center_std_dev = 3.0		data, labels = create_dataset(num_clusters, num_dimensions, num_points, cluster_std_devs, center_std_dev)		# Step 2: Plot the original data (2D projection using PCA)	print("Original Data (2D projection)")	plot_dataset(data, labels)		# Step 3: Introduce 10% missing values	nan_data = data.copy()	num_elements = nan_data.size	num_nan = int(0.1 * num_elements)  # 10% of the data		# Randomly choose 10% of indices to set as NaN	nan_indices = np.random.choice(num_elements, num_nan, replace=False)	nan_data.ravel()[nan_indices] = np.nan		# Step 4: Impute missing values using rank-4 SVD approximation	imputed_data = impute_missing_values(nan_data, rank=4)		# Step 5: Plot the imputed data (2D projection using PCA)	print("Imputed Data (2D projection)")	plot_dataset(imputed_data, labels)
2024-11-17-14-12-16 104 7 import matplotlib.pyplot as plt	from sklearn.decomposition import PCA		def plot_dataset(data: np.ndarray, labels: np.ndarray) -> None:	    if (data.ndim == 1):	        return	    data_2d = PCA(n_components=2).fit_transform(data)	    plt.scatter(*data_2d.T, c=labels, alpha=0.5)	    plt.axis('equal')	    plt.xlabel('First PCA component')	    plt.ylabel('Second PCA component')	    plt.grid()	    plt.show()	    return		save_history()
2024-11-17-14-12-16 104 8 # Step 1: Create a 5-dimensional dataset with 3 clusters	num_clusters = 3	num_dimensions = 5	num_points = np.array([100, 100, 100])	cluster_std_devs = np.array([1.0, 1.0, 1.0])	center_std_dev = 3.0		data, labels = create_dataset(num_clusters, num_dimensions, num_points, cluster_std_devs, center_std_dev)		# Step 2: Plot the original data (2D projection using PCA)	print("Original Data (2D projection)")	plot_dataset(data, labels)		# Step 3: Introduce 10% missing values	nan_data = data.copy()	num_elements = nan_data.size	num_nan = int(0.1 * num_elements)  # 10% of the data		# Randomly choose 10% of indices to set as NaN	nan_indices = np.random.choice(num_elements, num_nan, replace=False)	nan_data.ravel()[nan_indices] = np.nan		# Step 4: Impute missing values using rank-4 SVD approximation	imputed_data = impute_missing_values(nan_data, rank=4)		# Step 5: Plot the imputed data (2D projection using PCA)	print("Imputed Data (2D projection)")	plot_dataset(imputed_data, labels)
2024-11-17-14-12-16 104 9 from sklearn.neighbors import NearestNeighbors		def construct_knn_graph(data: np.ndarray, k: int) -> np.ndarray:	    knn = NearestNeighbors(n_neighbors=k)	    knn.fit(data)	    distances, indices = knn.kneighbors(data)	    num_samples = data.shape[0]	    knn_graph = np.zeros((num_samples, num_samples))	    for i, (distances_vector, indices_vector) in enumerate(zip(distances, indices)):	        for distance, j in zip(distances_vector, indices_vector):	            knn_graph[i, j] = distance	    return knn_graph		X = [[0], [3], [1]]	k = 2	neigh = NearestNeighbors(n_neighbors=k)	neigh.fit(X)	print(neigh.kneighbors_graph(X, mode='distance').toarray())	print(construct_knn_graph(np.array(X), k))		save_history()
2024-11-17-14-12-16 104 10 from sklearn.neighbors import NearestNeighbors		def construct_knn_graph(data: np.ndarray, k: int) -> np.ndarray:	    knn = NearestNeighbors(n_neighbors=k)	    knn.fit(data)	    distances, indices = knn.kneighbors(data)	    num_samples = data.shape[0]	    knn_graph = np.zeros((num_samples, num_samples))	    for i, (distances_vector, indices_vector) in enumerate(zip(distances, indices)):	        for distance, j in zip(distances_vector, indices_vector):	            knn_graph[i, j] = distance	    print(knn_graph)	    return knn_graph		X = [[0], [3], [1]]	k = 2	neigh = NearestNeighbors(n_neighbors=k)	neigh.fit(X)	print(neigh.kneighbors_graph(X, mode='distance').toarray())	print(construct_knn_graph(np.array(X), k))		save_history()
