2024-11-17-16-09-23 105 1 get_ipython().run_line_magic('matplotlib', 'inline')	import matplotlib.pyplot as plt	from typing import Tuple	import numpy as np	import scipy as sp	from submission_utils import save_history, check_and_prepare_for_submission	# import warnings filter	from warnings import simplefilter	# ignore all future warnings	simplefilter(action='ignore', category=FutureWarning)
2024-11-17-16-09-23 105 2 def create_dataset(num_clusters: int, 	                   num_dimensions: int, 	                   num_points: np.ndarray, 	                   cluster_std_devs: np.ndarray, 	                   center_std_dev: float) -> Tuple[np.ndarray, np.ndarray]:	    data = []	    labels = []	    origin = np.zeros((num_dimensions,))	    cluster_centres = np.random.normal(loc=origin, scale=center_std_dev, size=(num_clusters, num_dimensions))	    for i in range(num_clusters):	        cluster_points = np.random.normal(loc=cluster_centres[i], scale=cluster_std_devs[i], size=(num_points[i], num_dimensions))	        for point in cluster_points:	            data.append(point)	            labels.append(i)	    data = np.array(data)	    labels = np.array(labels)	    return (data, labels)	# data, labels = create_dataset(4, 5, np.array([4, 4, 4, 4]), np.array([1, 3, 5, 7, 9]), 10)		save_history()
2024-11-17-16-09-23 105 3 # This cell is reserved for the unit tests. Do not consider this cell. 
2024-11-17-16-09-23 105 4 # This cell is reserved for the unit tests. Do not consider this cell. 
2024-11-17-16-09-23 105 5 # from sklearn.decomposition import PCA	# from sklearn.datasets import load_iris		def pca_with_svd(data: np.ndarray, n_components: int) -> np.ndarray:	    centred_data = data - np.mean(data, axis=0)	    u, s, vh = np.linalg.svd(centred_data)	    transformed_data = centred_data.dot(vh.T[:, :n_components])	    return transformed_data		# df = load_iris()	# X = df.data	# pca = PCA(n_components=2)	# X_proj = pca.fit_transform(X)	# print(X_proj)	# Y = df.data	# print(pca_with_svd(Y, 2))	save_history()
2024-11-17-16-09-23 105 6 # This cell is reserved for the unit tests. Do not consider this cell. 
2024-11-17-16-09-23 105 7 import matplotlib.pyplot as plt	from sklearn.decomposition import PCA		def plot_dataset(data: np.ndarray, labels: np.ndarray) -> None:	    if (data.ndim == 1):	        return	    data_2d = PCA(n_components=2).fit_transform(data)	    plt.scatter(*data_2d.T, c=labels, alpha=0.5)	    plt.axis('equal')	    plt.xlabel('First PCA component')	    plt.ylabel('Second PCA component')	    plt.grid()	    plt.show()	    return		save_history()
2024-11-17-16-09-23 105 8 # This cell is reserved for the unit tests. Do not consider this cell. 
2024-11-17-16-09-23 105 9 def impute_missing_values(data: np.ndarray, rank: int) -> np.ndarray:	    no_nan_data = np.nan_to_num(data, nan=0)	    u, s, vh = np.linalg.svd(no_nan_data)	    s = np.diag(s)	    num_cols_u = u.shape[1]	    if num_cols_u > rank:	        u_trunc = u[:, :rank]	    else:	        u_trunc = u	    num_cols_vh = vh.shape[1]	    if num_cols_vh > rank:	        vh_trunc = vh[:rank, :]	    else:	        vh_trunc = vh	    num_sinuglar_values = s.shape[0]	    if num_sinuglar_values > rank:	        s_trunc = s[:rank, :rank]	    else:	        s_trunc = s	    trunc_svd_data = np.dot(np.dot(u_trunc, s_trunc), vh_trunc)	    nan_mask = np.isnan(data)	    nan_indices = np.where(nan_mask)	    imputed_data = data.copy()	    for row_ind, col_ind in zip(nan_indices[0], nan_indices[1]):	        impute_value = trunc_svd_data[row_ind, col_ind]	        imputed_data[row_ind, col_ind] = impute_value	    return imputed_data		save_history()
2024-11-17-16-09-23 105 10 # This cell is reserved for the unit tests. Do not consider this cell. 
2024-11-17-16-09-23 105 11 # Step 1: Create a 5-dimensional dataset with 3 clusters	num_clusters = 3	num_dimensions = 5	num_points = np.array([100, 100, 100])	cluster_std_devs = np.array([1.0, 1.0, 1.0])	center_std_dev = 3.0		data, labels = create_dataset(num_clusters, num_dimensions, num_points, cluster_std_devs, center_std_dev)		# Step 2: Plot the original data (2D projection using PCA)	print("Original Data (2D projection)")	plot_dataset(data, labels)		# Step 3: Introduce 10% missing values	nan_data = data.copy()	num_elements = nan_data.size	num_nan = int(0.1 * num_elements)  # 10% of the data		# Randomly choose 10% of indices to set as NaN	nan_indices = np.random.choice(num_elements, num_nan, replace=False)	nan_data.ravel()[nan_indices] = np.nan		# Step 4: Impute missing values using rank-4 SVD approximation	imputed_data = impute_missing_values(nan_data, rank=4)		# Step 5: Plot the imputed data (2D projection using PCA)	print("Imputed Data (2D projection)")	plot_dataset(imputed_data, labels)
2024-11-17-16-09-23 105 12 from sklearn.neighbors import NearestNeighbors		def construct_knn_graph(data: np.ndarray, k: int) -> np.ndarray:	    knn = NearestNeighbors(n_neighbors=k)	    knn.fit(data)	    distances, indices = knn.kneighbors(data)	    num_samples = data.shape[0]	    knn_graph = np.zeros((num_samples, num_samples))	    for i, (distances_vector, indices_vector) in enumerate(zip(distances, indices)):	        for distance, j in zip(distances_vector, indices_vector):	            knn_graph[i, j] = distance	            knn_graph[j, i] = distance	    return knn_graph		# X = [[0], [3], [1]]	# k = 2	# neigh = NearestNeighbors(n_neighbors=k)	# neigh.fit(X)	# print(neigh.kneighbors_graph(X, mode='distance').toarray())	# print(construct_knn_graph(np.array(X), k))		save_history()
2024-11-17-16-09-23 105 13 from scipy.sparse import csr_matrix	from scipy.sparse.csgraph import dijkstra		def compute_geodesic_distances(knn_graph: np.ndarray) -> np.ndarray:	    num_samples = knn_graph.shape[0]	    geodesic_distances = np.zeros((num_samples, num_samples))	    knn_cs_graph = csr_matrix(knn_graph)	    for i in range(num_samples):	        distances_vector = dijkstra(csgraph=knn_cs_graph, indices=i, directed=False)	        geodesic_distances[i] = distances_vector	    return geodesic_distances		# graph = np.array([	#     [0, 1, 2, 0],	#     [0, 0, 0, 1],	#     [0, 0, 0, 3],	#     [0, 0, 0, 0]	#     ])	# print(compute_geodesic_distances(graph))	# print(dijkstra(csgraph=csr_matrix(graph)))		save_history()
2024-11-17-16-09-23 105 14 from sklearn.manifold import MDS		def apply_mds(geodesic_distances: np.ndarray, n_components: int) -> np.ndarray:	    embedding = MDS(n_components=n_components, dissimilarity='precomputed')	    geodesic_distances[geodesic_distances == np.inf] = 1e9	    reduced_data = embedding.fit_transform(geodesic_distances)	    return reduced_data		save_history()
2024-11-17-16-09-23 105 15 def isomap(data: np.ndarray, n_components: int, k: int) -> np.ndarray:	    knn_graph = construct_knn_graph(data, k)	    geodesic_distances = compute_geodesic_distances(knn_graph)	    reduced_data = apply_mds(geodesic_distances, n_components)	    return reduced_data		# from sklearn.datasets import load_digits	# from sklearn.manifold import Isomap	# X, _ = load_digits(return_X_y=True)	# data = X[:100]	# embedding = Isomap(n_components=2)	# print(embedding.fit_transform(data))	# print(isomap(data, 2, 1))		save_history()
2024-11-17-16-09-23 105 16 # This cell is reserved for the unit tests. Do not consider this cell. 
2024-11-17-16-09-23 105 17 # This cell is reserved for the unit tests. Do not consider this cell. 
2024-11-17-16-09-23 105 18 # This cell is reserved for the unit tests. Do not consider this cell. 
2024-11-17-16-09-23 105 19 # This cell is reserved for the unit tests. Do not consider this cell. 
2024-11-17-16-09-23 105 20 def initialize_centroids(data: np.ndarray, k: int) -> np.ndarray:	    centroids = []	    num_samples = data.shape[0]	    for _ in range(k):	        rand_ind = np.random.choice(num_samples)	        rand_data_point = data[rand_ind]	        centroids.append(rand_data_point)	    centroids = np.array(centroids)	    return centroids		# dummies = np.arange(99)	# dummies.shape = (-1, 3)	# print(initialize_centroids(dummies, 5))		save_history()
2024-11-17-16-09-23 105 21 # This cell is reserved for the unit tests. Do not consider this cell. 
2024-11-17-16-09-23 105 22 def assign_clusters(data: np.ndarray, centroids: np.ndarray) -> np.ndarray:	    num_clusters = centroids.shape[0]	    labels = []	    for data_point in data:	        best_centroid_ind = 0	        best_euclidean_distance = 1e9	        for i in range (num_clusters):	            centroid = centroids[i]	            euclidean_distance = sum((centroid_val - data_point_val) ** 2 for centroid_val, data_point_val in zip(centroid, data_point)) ** 0.5	            if (euclidean_distance < best_euclidean_distance):	                best_euclidean_distance = euclidean_distance	                best_centroid_ind = i	        labels.append(best_centroid_ind)	    labels = np.array(labels)	    return labels		# dummies = np.random.normal(scale=20, size=(100, 3))	# print(dummies)	# print(assign_clusters(dummies, initialize_centroids(dummies, 5)))		save_history()
2024-11-17-16-09-23 105 23 # This cell is reserved for the unit tests. Do not consider this cell. 
2024-11-17-16-09-23 105 24 def update_centroids(data: np.ndarray, labels: np.ndarray, k: int) -> np.ndarray:	    clusters = {}	    for i in range(k):	        clusters[f"{i}"] = []	    for i, data_point in enumerate(data):	        clusters[f"{labels[i]}"].append(data_point)	    new_centroids = []	    for i in range(k):	        cluster = np.array(clusters[f"{i}"])	        new_centroids.append(np.array([np.mean(feature) for feature in cluster.T]))		    new_centroids = np.array(new_centroids)	    return new_centroids		# dummies = np.random.normal(scale=20, size=(100, 3))	# labels = assign_clusters(dummies, initialize_centroids(dummies, 5))	# print(update_centroids(dummies, labels, 5))		save_history()
2024-11-17-16-09-23 105 25 # This cell is reserved for the unit tests. Do not consider this cell. 
2024-11-17-16-09-23 105 26 def kmeans(data: np.ndarray, k: int, max_iter: int = 100, tol: float = 1e-4) -> Tuple[np.ndarray, np.ndarray]:	    centroids = initialize_centroids(data, k)	    labels = assign_clusters(data, centroids)	    max_movement = 1e9	    while (tol < max_movement and max_iter > 0):	        old_centroids = centroids.copy()	        centroids = update_centroids(data, labels, k)	        movements = []	        for i in range(centroids.shape[0]):	            old_centroid = old_centroids[i]	            centroid = centroids[i]	            movements.append(sum((old_val - val) ** 2 for old_val, val in zip(old_centroid, centroid)) ** 0.5)	        max_movement = max(movements)	        labels = assign_clusters(data, centroids)	        max_iter -= 1	    return (centroids, labels)		# true_centroids = [[0, 0], [-5, -5], [5, 5]]	# clusters = [np.random.randn(100, 2) + true_centroid for true_centroid in true_centroids]	# data = np.vstack(clusters)	# print(kmeans(data, len(true_centroids)))		save_history()
2024-11-17-16-09-23 105 27 # This cell is reserved for the unit tests. Do not consider this cell. 
2024-11-17-16-09-23 105 28 def within_cluster_distances(data: np.ndarray, labels: np.ndarray) -> np.ndarray:	    num_samples = data.shape[0]	    within_distances = np.zeros((num_samples,))	    for result_ind, (data_point, cluster_ind) in enumerate(zip(data, labels)):	        cluster_points_inds = np.argwhere(labels == cluster_ind)	        cluster_points = data[cluster_points_inds]	        a_of_i_frac = 1 / (len(cluster_points) - 1)	        a_of_i_sum = 0	        for other_data_point in cluster_points:	            other_data_point.shape = (other_data_point.shape[1],)	            if not np.array_equal(data_point, other_data_point):	                a_of_i_sum += (sum((i - j) ** 2 for i, j in zip(data_point, other_data_point))) ** 0.5	        a_of_i = a_of_i_frac * a_of_i_sum	        within_distances[result_ind] = a_of_i	    return within_distances		# data, labels = create_dataset(4, 5, np.array([4, 4, 4, 4]), np.array([1, 3, 5, 7, 9]), 10)	# print(within_cluster_distances(data, labels))		save_history()
2024-11-17-16-09-23 105 29 # This cell is reserved for the unit tests. Do not consider this cell. 
2024-11-17-16-09-23 105 30 def nearest_cluster_distances(data: np.ndarray, labels: np.ndarray) -> np.ndarray:	    num_samples = data.shape[0]	    nearest_distances = np.zeros((num_samples,))	    for result_ind, (data_point, cluster_ind) in enumerate(zip(data, labels)):	        nearest_distance = 1e9	        num_clusters = max(labels)	        for other_cluster_ind in [ind for ind in range(num_clusters) if ind != cluster_ind]:	            other_cluster_points_inds = np.argwhere(labels == other_cluster_ind)	            other_cluster_points = data[other_cluster_points_inds]	            b_of_i_frac = 1 / len(other_cluster_points_inds)	            b_of_i_sum = 0	            for other_data_point in other_cluster_points:	                other_data_point.shape = (other_data_point.shape[1],)	                b_of_i_sum += (sum((i - j) ** 2 for i, j in zip(data_point, other_data_point))) ** 0.5	            b_of_i = b_of_i_frac * b_of_i_sum	            if b_of_i < nearest_distance:	                nearest_distance = b_of_i	        nearest_distances[result_ind] = nearest_distance	    return nearest_distances		# data, labels = create_dataset(4, 5, np.array([4, 4, 4, 4]), np.array([1, 3, 5, 7, 9]), 10)	# print(nearest_cluster_distances(data, labels))		save_history()
2024-11-17-16-09-23 105 31 # This cell is reserved for the unit tests. Do not consider this cell. 
2024-11-17-16-09-23 105 32 def silhouette_score(data: np.ndarray, labels: np.ndarray) -> Tuple[np.ndarray, float]:	    num_samples = len(labels)	    scores = np.zeros(num_samples)	    a_of_is = within_cluster_distances(data, labels)	    b_of_is = nearest_cluster_distances(data, labels)	    for i in range(num_samples):	        a_of_i = a_of_is[i]	        b_of_i = b_of_is[i]	        score = (b_of_i - a_of_i) / max(a_of_i, b_of_i)	        scores[i] = score	    mean_score = np.mean(scores)	    return scores, mean_score		# data, labels = create_dataset(4, 5, np.array([4, 4, 4, 4]), np.array([1, 1, 1, 1]), 10)	# print(silhouette_score(data, labels))		save_history()
2024-11-17-16-09-23 105 33 # This cell is reserved for the unit tests. Do not consider this cell. 
2024-11-17-16-09-23 105 34 import numpy as np	import matplotlib.pyplot as plt		# Step 1: Generate 5-dimensional data with overlapping clusters	# We create 3 clusters with significant overlap	np.random.seed(42)	num_clusters = 3	num_dimensions = 5	num_points = np.array([100, 100, 100])	cluster_std_devs = np.array([1.5, 2.5, 2.5])  # High std dev for overlap	center_std_dev = 5.0  # Spread out the cluster centers more		# Generate the dataset	data, labels = create_dataset(num_clusters, num_dimensions, num_points, cluster_std_devs, center_std_dev)		# Step 2: Introduce missing values into the dataset (10% of the data)	nan_data = data.copy()	num_elements = nan_data.size	num_nan = int(0.1 * num_elements)  # 10% of the data	nan_indices = np.random.choice(num_elements, num_nan, replace=False)	nan_data.ravel()[nan_indices] = np.nan		# Step 3: Impute missing values using low-rank SVD with rank 4	imputed_data = impute_missing_values(nan_data, rank=4)		# Step 4: Apply ISOMAP to reduce to 2D for visualization	n_components = 2	data_2d = isomap(imputed_data, n_components=n_components, k=5)		# Step 5: Apply kmeans clustering and calculate silhouette scores for each cluster count	n_clusters_range = range(2, 9)	silhouette_scores = []		for n_clusters in n_clusters_range:	    # Apply kmeans clustering to the reduced data	    centroids, cluster_labels = kmeans(data_2d, k=n_clusters, max_iter=100, tol=1e-4)	    	    # Calculate the silhouette score for the current number of clusters	    _, mean_silhouette_score = silhouette_score(data_2d, cluster_labels)	    silhouette_scores.append(mean_silhouette_score)		# Step 6: Identify the best number of clusters based on silhouette score	best_n_clusters = n_clusters_range[np.argmax(silhouette_scores)]	best_silhouette_score = max(silhouette_scores)		# Print the results	print(f"Best number of clusters: {best_n_clusters}")	print(f"Best silhouette score: {best_silhouette_score:.3f}")		# Plot silhouette scores vs. number of clusters	plt.figure(figsize=(10, 6))	plt.plot(n_clusters_range, silhouette_scores, marker='o')	plt.title("Silhouette Scores for Different Numbers of Clusters")	plt.xlabel("Number of Clusters")	plt.ylabel("Mean Silhouette Score")	plt.show()
2024-11-17-16-09-23 105 35 # Do not consider the next cell.	# You do not have to do anything for the next cell.
2024-11-17-16-09-23 105 36 # YOUR CODE HERE	raise NotImplementedError()
2024-11-17-16-09-23 105 37 import numpy as np	import matplotlib.pyplot as plt		# Step 1: Generate 5-dimensional data with overlapping clusters	# We create 3 clusters with significant overlap	np.random.seed(42)	num_clusters = 3	num_dimensions = 5	num_points = np.array([100, 100, 100])	cluster_std_devs = np.array([1.5, 2.5, 2.5])  # High std dev for overlap	center_std_dev = 5.0  # Spread out the cluster centers more		# Generate the dataset	data, labels = create_dataset(num_clusters, num_dimensions, num_points, cluster_std_devs, center_std_dev)		# Step 2: Introduce missing values into the dataset (10% of the data)	nan_data = data.copy()	num_elements = nan_data.size	num_nan = int(0.1 * num_elements)  # 10% of the data	nan_indices = np.random.choice(num_elements, num_nan, replace=False)	nan_data.ravel()[nan_indices] = np.nan		# Step 3: Impute missing values using low-rank SVD with rank 4	imputed_data = impute_missing_values(nan_data, rank=4)		# Step 4: Apply ISOMAP to reduce to 2D for visualization	n_components = 2	data_2d = isomap(imputed_data, n_components=n_components, k=5)		# Step 5: Apply kmeans clustering and calculate silhouette scores for each cluster count	n_clusters_range = range(2, 9)	silhouette_scores = []		for n_clusters in n_clusters_range:	    # Apply kmeans clustering to the reduced data	    centroids, cluster_labels = kmeans(data_2d, k=n_clusters, max_iter=100, tol=1e-4)	    	    # Calculate the silhouette score for the current number of clusters	    _, mean_silhouette_score = silhouette_score(data_2d, cluster_labels)	    silhouette_scores.append(mean_silhouette_score)		# Step 6: Identify the best number of clusters based on silhouette score	best_n_clusters = n_clusters_range[np.argmax(silhouette_scores)]	best_silhouette_score = max(silhouette_scores)		# Print the results	print(f"Best number of clusters: {best_n_clusters}")	print(f"Best silhouette score: {best_silhouette_score:.3f}")		# Plot silhouette scores vs. number of clusters	plt.figure(figsize=(10, 6))	plt.plot(n_clusters_range, silhouette_scores, marker='o')	plt.title("Silhouette Scores for Different Numbers of Clusters")	plt.xlabel("Number of Clusters")	plt.ylabel("Mean Silhouette Score")	plt.show()
2024-11-17-16-09-23 105 38 from sklearn.manifold import MDS		def apply_mds(geodesic_distances: np.ndarray, n_components: int) -> np.ndarray:	    embedding = MDS(n_components=n_components)	    geodesic_distances[geodesic_distances == np.inf] = 1e9	    reduced_data = embedding.fit_transform(geodesic_distances)	    return reduced_data		save_history()
2024-11-17-16-09-23 105 39 import numpy as np	import matplotlib.pyplot as plt		# Step 1: Generate 5-dimensional data with overlapping clusters	# We create 3 clusters with significant overlap	np.random.seed(42)	num_clusters = 3	num_dimensions = 5	num_points = np.array([100, 100, 100])	cluster_std_devs = np.array([1.5, 2.5, 2.5])  # High std dev for overlap	center_std_dev = 5.0  # Spread out the cluster centers more		# Generate the dataset	data, labels = create_dataset(num_clusters, num_dimensions, num_points, cluster_std_devs, center_std_dev)		# Step 2: Introduce missing values into the dataset (10% of the data)	nan_data = data.copy()	num_elements = nan_data.size	num_nan = int(0.1 * num_elements)  # 10% of the data	nan_indices = np.random.choice(num_elements, num_nan, replace=False)	nan_data.ravel()[nan_indices] = np.nan		# Step 3: Impute missing values using low-rank SVD with rank 4	imputed_data = impute_missing_values(nan_data, rank=4)		# Step 4: Apply ISOMAP to reduce to 2D for visualization	n_components = 2	data_2d = isomap(imputed_data, n_components=n_components, k=5)		# Step 5: Apply kmeans clustering and calculate silhouette scores for each cluster count	n_clusters_range = range(2, 9)	silhouette_scores = []		for n_clusters in n_clusters_range:	    # Apply kmeans clustering to the reduced data	    centroids, cluster_labels = kmeans(data_2d, k=n_clusters, max_iter=100, tol=1e-4)	    	    # Calculate the silhouette score for the current number of clusters	    _, mean_silhouette_score = silhouette_score(data_2d, cluster_labels)	    silhouette_scores.append(mean_silhouette_score)		# Step 6: Identify the best number of clusters based on silhouette score	best_n_clusters = n_clusters_range[np.argmax(silhouette_scores)]	best_silhouette_score = max(silhouette_scores)		# Print the results	print(f"Best number of clusters: {best_n_clusters}")	print(f"Best silhouette score: {best_silhouette_score:.3f}")		# Plot silhouette scores vs. number of clusters	plt.figure(figsize=(10, 6))	plt.plot(n_clusters_range, silhouette_scores, marker='o')	plt.title("Silhouette Scores for Different Numbers of Clusters")	plt.xlabel("Number of Clusters")	plt.ylabel("Mean Silhouette Score")	plt.show()
2024-11-17-16-09-23 105 40 from sklearn.neighbors import NearestNeighbors		def construct_knn_graph(data: np.ndarray, k: int) -> np.ndarray:	    knn = NearestNeighbors(n_neighbors=k)	    knn.fit(data)	    distances, indices = knn.kneighbors(data)	    num_samples = data.shape[0]	    knn_graph = np.zeros((num_samples, num_samples))	    for i, (distances_vector, indices_vector) in enumerate(zip(distances, indices)):	        for distance, j in zip(distances_vector, indices_vector):	            knn_graph[i, j] = distance	    return knn_graph		# X = [[0], [3], [1]]	# k = 2	# neigh = NearestNeighbors(n_neighbors=k)	# neigh.fit(X)	# print(neigh.kneighbors_graph(X, mode='distance').toarray())	# print(construct_knn_graph(np.array(X), k))		save_history()
2024-11-17-16-09-23 105 41 from scipy.sparse import csr_matrix	from scipy.sparse.csgraph import dijkstra		def compute_geodesic_distances(knn_graph: np.ndarray) -> np.ndarray:	    num_samples = knn_graph.shape[0]	    geodesic_distances = np.zeros((num_samples, num_samples))	    knn_cs_graph = csr_matrix(knn_graph)	    for i in range(num_samples):	        distances_vector = dijkstra(csgraph=knn_cs_graph, indices=i)	        geodesic_distances[i] = distances_vector	    return geodesic_distances		# graph = np.array([	#     [0, 1, 2, 0],	#     [0, 0, 0, 1],	#     [0, 0, 0, 3],	#     [0, 0, 0, 0]	#     ])	# print(compute_geodesic_distances(graph))	# print(dijkstra(csgraph=csr_matrix(graph)))		save_history()
2024-11-17-16-09-23 105 42 import numpy as np	import matplotlib.pyplot as plt		# Step 1: Generate 5-dimensional data with overlapping clusters	# We create 3 clusters with significant overlap	np.random.seed(42)	num_clusters = 3	num_dimensions = 5	num_points = np.array([100, 100, 100])	cluster_std_devs = np.array([1.5, 2.5, 2.5])  # High std dev for overlap	center_std_dev = 5.0  # Spread out the cluster centers more		# Generate the dataset	data, labels = create_dataset(num_clusters, num_dimensions, num_points, cluster_std_devs, center_std_dev)		# Step 2: Introduce missing values into the dataset (10% of the data)	nan_data = data.copy()	num_elements = nan_data.size	num_nan = int(0.1 * num_elements)  # 10% of the data	nan_indices = np.random.choice(num_elements, num_nan, replace=False)	nan_data.ravel()[nan_indices] = np.nan		# Step 3: Impute missing values using low-rank SVD with rank 4	imputed_data = impute_missing_values(nan_data, rank=4)		# Step 4: Apply ISOMAP to reduce to 2D for visualization	n_components = 2	data_2d = isomap(imputed_data, n_components=n_components, k=5)		# Step 5: Apply kmeans clustering and calculate silhouette scores for each cluster count	n_clusters_range = range(2, 9)	silhouette_scores = []		for n_clusters in n_clusters_range:	    # Apply kmeans clustering to the reduced data	    centroids, cluster_labels = kmeans(data_2d, k=n_clusters, max_iter=100, tol=1e-4)	    	    # Calculate the silhouette score for the current number of clusters	    _, mean_silhouette_score = silhouette_score(data_2d, cluster_labels)	    silhouette_scores.append(mean_silhouette_score)		# Step 6: Identify the best number of clusters based on silhouette score	best_n_clusters = n_clusters_range[np.argmax(silhouette_scores)]	best_silhouette_score = max(silhouette_scores)		# Print the results	print(f"Best number of clusters: {best_n_clusters}")	print(f"Best silhouette score: {best_silhouette_score:.3f}")		# Plot silhouette scores vs. number of clusters	plt.figure(figsize=(10, 6))	plt.plot(n_clusters_range, silhouette_scores, marker='o')	plt.title("Silhouette Scores for Different Numbers of Clusters")	plt.xlabel("Number of Clusters")	plt.ylabel("Mean Silhouette Score")	plt.show()
2024-11-17-16-09-23 105 43 def within_cluster_distances(data: np.ndarray, labels: np.ndarray) -> np.ndarray:	    num_samples = data.shape[0]	    within_distances = np.zeros((num_samples,))	    for result_ind, (data_point, cluster_ind) in enumerate(zip(data, labels)):	        cluster_points_inds = np.argwhere(labels == cluster_ind)	        cluster_points = data[cluster_points_inds]	        if len(cluster_points) > 1:	            a_of_i_frac = 1 / (len(cluster_points) - 1)	            a_of_i_sum = 0	            for other_data_point in cluster_points:	                other_data_point.shape = (other_data_point.shape[1],)	                if not np.array_equal(data_point, other_data_point):	                    a_of_i_sum += (sum((i - j) ** 2 for i, j in zip(data_point, other_data_point))) ** 0.5	            a_of_i = a_of_i_frac * a_of_i_sum	            within_distances[result_ind] = a_of_i	        else:	            within_distances[result_ind] = 0	    return within_distances		# data, labels = create_dataset(4, 5, np.array([4, 4, 4, 4]), np.array([1, 3, 5, 7, 9]), 10)	# print(within_cluster_distances(data, labels))		save_history()
2024-11-17-16-09-23 105 44 import numpy as np	import matplotlib.pyplot as plt		# Step 1: Generate 5-dimensional data with overlapping clusters	# We create 3 clusters with significant overlap	np.random.seed(42)	num_clusters = 3	num_dimensions = 5	num_points = np.array([100, 100, 100])	cluster_std_devs = np.array([1.5, 2.5, 2.5])  # High std dev for overlap	center_std_dev = 5.0  # Spread out the cluster centers more		# Generate the dataset	data, labels = create_dataset(num_clusters, num_dimensions, num_points, cluster_std_devs, center_std_dev)		# Step 2: Introduce missing values into the dataset (10% of the data)	nan_data = data.copy()	num_elements = nan_data.size	num_nan = int(0.1 * num_elements)  # 10% of the data	nan_indices = np.random.choice(num_elements, num_nan, replace=False)	nan_data.ravel()[nan_indices] = np.nan		# Step 3: Impute missing values using low-rank SVD with rank 4	imputed_data = impute_missing_values(nan_data, rank=4)		# Step 4: Apply ISOMAP to reduce to 2D for visualization	n_components = 2	data_2d = isomap(imputed_data, n_components=n_components, k=5)		# Step 5: Apply kmeans clustering and calculate silhouette scores for each cluster count	n_clusters_range = range(2, 9)	silhouette_scores = []		for n_clusters in n_clusters_range:	    # Apply kmeans clustering to the reduced data	    centroids, cluster_labels = kmeans(data_2d, k=n_clusters, max_iter=100, tol=1e-4)	    	    # Calculate the silhouette score for the current number of clusters	    _, mean_silhouette_score = silhouette_score(data_2d, cluster_labels)	    silhouette_scores.append(mean_silhouette_score)		# Step 6: Identify the best number of clusters based on silhouette score	best_n_clusters = n_clusters_range[np.argmax(silhouette_scores)]	best_silhouette_score = max(silhouette_scores)		# Print the results	print(f"Best number of clusters: {best_n_clusters}")	print(f"Best silhouette score: {best_silhouette_score:.3f}")		# Plot silhouette scores vs. number of clusters	plt.figure(figsize=(10, 6))	plt.plot(n_clusters_range, silhouette_scores, marker='o')	plt.title("Silhouette Scores for Different Numbers of Clusters")	plt.xlabel("Number of Clusters")	plt.ylabel("Mean Silhouette Score")	plt.show()
2024-11-17-16-09-23 105 45 from sklearn.manifold import MDS		def apply_mds(geodesic_distances: np.ndarray, n_components: int) -> np.ndarray:	    embedding = MDS(n_components=n_components)	    geodesic_distances[geodesic_distances == np.inf] = 1e9	    reduced_data = embedding.fit_transform(geodesic_distances)	    return reduced_data		save_history()
2024-11-17-16-09-23 105 46 import numpy as np	import matplotlib.pyplot as plt		# Step 1: Generate 5-dimensional data with overlapping clusters	# We create 3 clusters with significant overlap	np.random.seed(42)	num_clusters = 3	num_dimensions = 5	num_points = np.array([100, 100, 100])	cluster_std_devs = np.array([1.5, 2.5, 2.5])  # High std dev for overlap	center_std_dev = 5.0  # Spread out the cluster centers more		# Generate the dataset	data, labels = create_dataset(num_clusters, num_dimensions, num_points, cluster_std_devs, center_std_dev)		# Step 2: Introduce missing values into the dataset (10% of the data)	nan_data = data.copy()	num_elements = nan_data.size	num_nan = int(0.1 * num_elements)  # 10% of the data	nan_indices = np.random.choice(num_elements, num_nan, replace=False)	nan_data.ravel()[nan_indices] = np.nan		# Step 3: Impute missing values using low-rank SVD with rank 4	imputed_data = impute_missing_values(nan_data, rank=4)		# Step 4: Apply ISOMAP to reduce to 2D for visualization	n_components = 2	data_2d = isomap(imputed_data, n_components=n_components, k=5)		# Step 5: Apply kmeans clustering and calculate silhouette scores for each cluster count	n_clusters_range = range(2, 9)	silhouette_scores = []		for n_clusters in n_clusters_range:	    # Apply kmeans clustering to the reduced data	    centroids, cluster_labels = kmeans(data_2d, k=n_clusters, max_iter=100, tol=1e-4)	    	    # Calculate the silhouette score for the current number of clusters	    _, mean_silhouette_score = silhouette_score(data_2d, cluster_labels)	    silhouette_scores.append(mean_silhouette_score)		# Step 6: Identify the best number of clusters based on silhouette score	best_n_clusters = n_clusters_range[np.argmax(silhouette_scores)]	best_silhouette_score = max(silhouette_scores)		# Print the results	print(f"Best number of clusters: {best_n_clusters}")	print(f"Best silhouette score: {best_silhouette_score:.3f}")		# Plot silhouette scores vs. number of clusters	plt.figure(figsize=(10, 6))	plt.plot(n_clusters_range, silhouette_scores, marker='o')	plt.title("Silhouette Scores for Different Numbers of Clusters")	plt.xlabel("Number of Clusters")	plt.ylabel("Mean Silhouette Score")	plt.show()
2024-11-17-16-09-23 105 47 from sklearn.manifold import MDS		def apply_mds(geodesic_distances: np.ndarray, n_components: int) -> np.ndarray:	    embedding = MDS(n_components=n_components, dissimilarity='precomputed')	    geodesic_distances[geodesic_distances == np.inf] = 1e9	    reduced_data = embedding.fit_transform(geodesic_distances)	    return reduced_data		save_history()
2024-11-17-16-09-23 105 48 from scipy.sparse import csr_matrix	from scipy.sparse.csgraph import dijkstra		def compute_geodesic_distances(knn_graph: np.ndarray) -> np.ndarray:	    num_samples = knn_graph.shape[0]	    geodesic_distances = np.zeros((num_samples, num_samples))	    knn_cs_graph = csr_matrix(knn_graph)	    for i in range(num_samples):	        distances_vector = dijkstra(csgraph=knn_cs_graph, directed=False, indices=i)	        geodesic_distances[i] = distances_vector	    return geodesic_distances		# graph = np.array([	#     [0, 1, 2, 0],	#     [0, 0, 0, 1],	#     [0, 0, 0, 3],	#     [0, 0, 0, 0]	#     ])	# print(compute_geodesic_distances(graph))	# print(dijkstra(csgraph=csr_matrix(graph)))		save_history()
2024-11-17-16-09-23 105 49 from sklearn.neighbors import NearestNeighbors		def construct_knn_graph(data: np.ndarray, k: int) -> np.ndarray:	    knn = NearestNeighbors(n_neighbors=k)	    knn.fit(data)	    distances, indices = knn.kneighbors(data)	    num_samples = data.shape[0]	    knn_graph = np.zeros((num_samples, num_samples))	    for i, (distances_vector, indices_vector) in enumerate(zip(distances, indices)):	        for distance, j in zip(distances_vector, indices_vector):	            knn_graph[i, j] = distance	            knn_graph[j, i] = distance	    return knn_graph		# X = [[0], [3], [1]]	# k = 2	# neigh = NearestNeighbors(n_neighbors=k)	# neigh.fit(X)	# print(neigh.kneighbors_graph(X, mode='distance').toarray())	# print(construct_knn_graph(np.array(X), k))		save_history()
2024-11-17-16-09-23 105 50 import numpy as np	import matplotlib.pyplot as plt		# Step 1: Generate 5-dimensional data with overlapping clusters	# We create 3 clusters with significant overlap	np.random.seed(42)	num_clusters = 3	num_dimensions = 5	num_points = np.array([100, 100, 100])	cluster_std_devs = np.array([1.5, 2.5, 2.5])  # High std dev for overlap	center_std_dev = 5.0  # Spread out the cluster centers more		# Generate the dataset	data, labels = create_dataset(num_clusters, num_dimensions, num_points, cluster_std_devs, center_std_dev)		# Step 2: Introduce missing values into the dataset (10% of the data)	nan_data = data.copy()	num_elements = nan_data.size	num_nan = int(0.1 * num_elements)  # 10% of the data	nan_indices = np.random.choice(num_elements, num_nan, replace=False)	nan_data.ravel()[nan_indices] = np.nan		# Step 3: Impute missing values using low-rank SVD with rank 4	imputed_data = impute_missing_values(nan_data, rank=4)		# Step 4: Apply ISOMAP to reduce to 2D for visualization	n_components = 2	data_2d = isomap(imputed_data, n_components=n_components, k=5)		# Step 5: Apply kmeans clustering and calculate silhouette scores for each cluster count	n_clusters_range = range(2, 9)	silhouette_scores = []		for n_clusters in n_clusters_range:	    # Apply kmeans clustering to the reduced data	    centroids, cluster_labels = kmeans(data_2d, k=n_clusters, max_iter=100, tol=1e-4)	    	    # Calculate the silhouette score for the current number of clusters	    _, mean_silhouette_score = silhouette_score(data_2d, cluster_labels)	    silhouette_scores.append(mean_silhouette_score)		# Step 6: Identify the best number of clusters based on silhouette score	best_n_clusters = n_clusters_range[np.argmax(silhouette_scores)]	best_silhouette_score = max(silhouette_scores)		# Print the results	print(f"Best number of clusters: {best_n_clusters}")	print(f"Best silhouette score: {best_silhouette_score:.3f}")		# Plot silhouette scores vs. number of clusters	plt.figure(figsize=(10, 6))	plt.plot(n_clusters_range, silhouette_scores, marker='o')	plt.title("Silhouette Scores for Different Numbers of Clusters")	plt.xlabel("Number of Clusters")	plt.ylabel("Mean Silhouette Score")	plt.show()
2024-11-17-16-09-23 105 51 import numpy as np	import matplotlib.pyplot as plt		# Step 1: Generate 5-dimensional data with overlapping clusters	# We create 3 clusters with significant overlap	np.random.seed(42)	num_clusters = 3	num_dimensions = 5	num_points = np.array([100, 100, 100])	cluster_std_devs = np.array([1.5, 2.5, 2.5])  # High std dev for overlap	center_std_dev = 5.0  # Spread out the cluster centers more		# Generate the dataset	data, labels = create_dataset(num_clusters, num_dimensions, num_points, cluster_std_devs, center_std_dev)		# Step 2: Introduce missing values into the dataset (10% of the data)	nan_data = data.copy()	num_elements = nan_data.size	num_nan = int(0.1 * num_elements)  # 10% of the data	nan_indices = np.random.choice(num_elements, num_nan, replace=False)	nan_data.ravel()[nan_indices] = np.nan		# Step 3: Impute missing values using low-rank SVD with rank 4	imputed_data = impute_missing_values(nan_data, rank=4)		# Step 4: Apply ISOMAP to reduce to 2D for visualization	n_components = 2	data_2d = isomap(imputed_data, n_components=n_components, k=5)		# Step 5: Apply kmeans clustering and calculate silhouette scores for each cluster count	n_clusters_range = range(2, 9)	silhouette_scores = []		for n_clusters in n_clusters_range:	    # Apply kmeans clustering to the reduced data	    centroids, cluster_labels = kmeans(data_2d, k=n_clusters, max_iter=100, tol=1e-4)	    plot_dataset(data_2d, cluster_labels)	    	    # Calculate the silhouette score for the current number of clusters	    _, mean_silhouette_score = silhouette_score(data_2d, cluster_labels)	    silhouette_scores.append(mean_silhouette_score)		# Step 6: Identify the best number of clusters based on silhouette score	best_n_clusters = n_clusters_range[np.argmax(silhouette_scores)]	best_silhouette_score = max(silhouette_scores)		# Print the results	print(f"Best number of clusters: {best_n_clusters}")	print(f"Best silhouette score: {best_silhouette_score:.3f}")		# Plot silhouette scores vs. number of clusters	plt.figure(figsize=(10, 6))	plt.plot(n_clusters_range, silhouette_scores, marker='o')	plt.title("Silhouette Scores for Different Numbers of Clusters")	plt.xlabel("Number of Clusters")	plt.ylabel("Mean Silhouette Score")	plt.show()
2024-11-17-16-09-23 105 52 import matplotlib.pyplot as plt	from sklearn.decomposition import PCA		def plot_dataset(data: np.ndarray, labels: np.ndarray) -> None:	    if data.ndim < 2:	        return	    elif data.ndim == 2:	        data_2d = data	    else:	        data_2d = PCA(n_components=2).fit_transform(data)	    plt.scatter(*data_2d.T, c=labels, alpha=0.5)	    plt.axis('equal')	    plt.xlabel('First PCA component')	    plt.ylabel('Second PCA component')	    plt.grid()	    plt.show()	    return		save_history()
2024-11-17-16-09-23 105 53 import numpy as np	import matplotlib.pyplot as plt		# Step 1: Generate 5-dimensional data with overlapping clusters	# We create 3 clusters with significant overlap	np.random.seed(42)	num_clusters = 3	num_dimensions = 5	num_points = np.array([100, 100, 100])	cluster_std_devs = np.array([1.5, 2.5, 2.5])  # High std dev for overlap	center_std_dev = 5.0  # Spread out the cluster centers more		# Generate the dataset	data, labels = create_dataset(num_clusters, num_dimensions, num_points, cluster_std_devs, center_std_dev)		# Step 2: Introduce missing values into the dataset (10% of the data)	nan_data = data.copy()	num_elements = nan_data.size	num_nan = int(0.1 * num_elements)  # 10% of the data	nan_indices = np.random.choice(num_elements, num_nan, replace=False)	nan_data.ravel()[nan_indices] = np.nan		# Step 3: Impute missing values using low-rank SVD with rank 4	imputed_data = impute_missing_values(nan_data, rank=4)		# Step 4: Apply ISOMAP to reduce to 2D for visualization	n_components = 2	data_2d = isomap(imputed_data, n_components=n_components, k=5)		# Step 5: Apply kmeans clustering and calculate silhouette scores for each cluster count	n_clusters_range = range(2, 9)	silhouette_scores = []		for n_clusters in n_clusters_range:	    # Apply kmeans clustering to the reduced data	    centroids, cluster_labels = kmeans(data_2d, k=n_clusters, max_iter=100, tol=1e-4)	    plot_dataset(data_2d, cluster_labels)	    	    # Calculate the silhouette score for the current number of clusters	    _, mean_silhouette_score = silhouette_score(data_2d, cluster_labels)	    silhouette_scores.append(mean_silhouette_score)		# Step 6: Identify the best number of clusters based on silhouette score	best_n_clusters = n_clusters_range[np.argmax(silhouette_scores)]	best_silhouette_score = max(silhouette_scores)		# Print the results	print(f"Best number of clusters: {best_n_clusters}")	print(f"Best silhouette score: {best_silhouette_score:.3f}")		# Plot silhouette scores vs. number of clusters	plt.figure(figsize=(10, 6))	plt.plot(n_clusters_range, silhouette_scores, marker='o')	plt.title("Silhouette Scores for Different Numbers of Clusters")	plt.xlabel("Number of Clusters")	plt.ylabel("Mean Silhouette Score")	plt.show()
2024-11-17-16-09-23 105 54 import matplotlib.pyplot as plt	from sklearn.decomposition import PCA		def plot_dataset(data: np.ndarray, labels: np.ndarray) -> None:	    if (data.ndim == 1):	        return	    data_2d = PCA(n_components=2).fit_transform(data)	    plt.scatter(*data_2d.T, c=labels, alpha=0.5)	    plt.axis('equal')	    plt.xlabel('First PCA component')	    plt.ylabel('Second PCA component')	    plt.grid()	    plt.show()	    return		save_history()
2024-11-17-16-09-23 105 55 import numpy as np	import matplotlib.pyplot as plt		# Step 1: Generate 5-dimensional data with overlapping clusters	# We create 3 clusters with significant overlap	np.random.seed(42)	num_clusters = 3	num_dimensions = 5	num_points = np.array([100, 100, 100])	cluster_std_devs = np.array([1.5, 2.5, 2.5])  # High std dev for overlap	center_std_dev = 5.0  # Spread out the cluster centers more		# Generate the dataset	data, labels = create_dataset(num_clusters, num_dimensions, num_points, cluster_std_devs, center_std_dev)		# Step 2: Introduce missing values into the dataset (10% of the data)	nan_data = data.copy()	num_elements = nan_data.size	num_nan = int(0.1 * num_elements)  # 10% of the data	nan_indices = np.random.choice(num_elements, num_nan, replace=False)	nan_data.ravel()[nan_indices] = np.nan		# Step 3: Impute missing values using low-rank SVD with rank 4	imputed_data = impute_missing_values(nan_data, rank=4)		# Step 4: Apply ISOMAP to reduce to 2D for visualization	n_components = 2	data_2d = isomap(imputed_data, n_components=n_components, k=5)		# Step 5: Apply kmeans clustering and calculate silhouette scores for each cluster count	n_clusters_range = range(2, 9)	silhouette_scores = []		for n_clusters in n_clusters_range:	    # Apply kmeans clustering to the reduced data	    centroids, cluster_labels = kmeans(data_2d, k=n_clusters, max_iter=100, tol=1e-4)	    plt.scatter(*data_2d.T, c=cluster_labels)	    plt.show()	    	    # Calculate the silhouette score for the current number of clusters	    _, mean_silhouette_score = silhouette_score(data_2d, cluster_labels)	    silhouette_scores.append(mean_silhouette_score)		# Step 6: Identify the best number of clusters based on silhouette score	best_n_clusters = n_clusters_range[np.argmax(silhouette_scores)]	best_silhouette_score = max(silhouette_scores)		# Print the results	print(f"Best number of clusters: {best_n_clusters}")	print(f"Best silhouette score: {best_silhouette_score:.3f}")		# Plot silhouette scores vs. number of clusters	plt.figure(figsize=(10, 6))	plt.plot(n_clusters_range, silhouette_scores, marker='o')	plt.title("Silhouette Scores for Different Numbers of Clusters")	plt.xlabel("Number of Clusters")	plt.ylabel("Mean Silhouette Score")	plt.show()
2024-11-17-16-09-23 105 56 import numpy as np	import matplotlib.pyplot as plt		# Step 1: Generate 5-dimensional data with overlapping clusters	# We create 3 clusters with significant overlap	np.random.seed(42)	num_clusters = 3	num_dimensions = 5	num_points = np.array([100, 100, 100])	cluster_std_devs = np.array([1.5, 2.5, 2.5])  # High std dev for overlap	center_std_dev = 5.0  # Spread out the cluster centers more		# Generate the dataset	data, labels = create_dataset(num_clusters, num_dimensions, num_points, cluster_std_devs, center_std_dev)		# Step 2: Introduce missing values into the dataset (10% of the data)	nan_data = data.copy()	num_elements = nan_data.size	num_nan = int(0.1 * num_elements)  # 10% of the data	nan_indices = np.random.choice(num_elements, num_nan, replace=False)	nan_data.ravel()[nan_indices] = np.nan		# Step 3: Impute missing values using low-rank SVD with rank 4	imputed_data = impute_missing_values(nan_data, rank=4)		# Step 4: Apply ISOMAP to reduce to 2D for visualization	n_components = 2	data_2d = isomap(imputed_data, n_components=n_components, k=5)		# Step 5: Apply kmeans clustering and calculate silhouette scores for each cluster count	n_clusters_range = range(2, 9)	silhouette_scores = []		for n_clusters in n_clusters_range:	    # Apply kmeans clustering to the reduced data	    centroids, cluster_labels = kmeans(data_2d, k=n_clusters, max_iter=100, tol=1e-4)	    plot = plt.scatter(*data_2d.T, c=cluster_labels)	    plt.colorbar(plot, label='Cluster')	    plt.show()	    	    # Calculate the silhouette score for the current number of clusters	    _, mean_silhouette_score = silhouette_score(data_2d, cluster_labels)	    silhouette_scores.append(mean_silhouette_score)		# Step 6: Identify the best number of clusters based on silhouette score	best_n_clusters = n_clusters_range[np.argmax(silhouette_scores)]	best_silhouette_score = max(silhouette_scores)		# Print the results	print(f"Best number of clusters: {best_n_clusters}")	print(f"Best silhouette score: {best_silhouette_score:.3f}")		# Plot silhouette scores vs. number of clusters	plt.figure(figsize=(10, 6))	plt.plot(n_clusters_range, silhouette_scores, marker='o')	plt.title("Silhouette Scores for Different Numbers of Clusters")	plt.xlabel("Number of Clusters")	plt.ylabel("Mean Silhouette Score")	plt.show()
2024-11-17-16-09-23 105 57 def kmeans(data: np.ndarray, k: int, max_iter: int = 100, tol: float = 1e-4) -> Tuple[np.ndarray, np.ndarray]:	    centroids = initialize_centroids(data, k)	    labels = assign_clusters(data, centroids)	    max_movement = 1e9	    while (tol < max_movement and max_iter > 0):	        old_centroids = centroids.copy()	        centroids = update_centroids(data, labels, k)	        movements = []	        for i in range(centroids.shape[0]):	            old_centroid = old_centroids[i]	            centroid = centroids[i]	            movements.append(sum((old_val - val) ** 2 for old_val, val in zip(old_centroid, centroid)) ** 0.5)	        max_movement = max(movements)	        labels = assign_clusters(data, centroids)	        max_iter -= 1	    return (centroids, labels)		true_centroids = [[0, 0], [-5, -5], [5, 5]]	clusters = [np.random.randn(100, 2) + true_centroid for true_centroid in true_centroids]	data = np.vstack(clusters)	print(kmeans(data, len(true_centroids)))		save_history()
2024-11-17-16-09-23 105 58 def kmeans(data: np.ndarray, k: int, max_iter: int = 100, tol: float = 1e-4) -> Tuple[np.ndarray, np.ndarray]:	    centroids = initialize_centroids(data, k)	    labels = assign_clusters(data, centroids)	    max_movement = 1e9	    while (tol < max_movement and max_iter > 0):	        old_centroids = centroids.copy()	        centroids = update_centroids(data, labels, k)	        movements = []	        for i in range(centroids.shape[0]):	            old_centroid = old_centroids[i]	            centroid = centroids[i]	            movements.append(sum((old_val - val) ** 2 for old_val, val in zip(old_centroid, centroid)) ** 0.5)	        max_movement = max(movements)	        labels = assign_clusters(data, centroids)	        max_iter -= 1	    return (centroids, labels)		# true_centroids = [[0, 0], [-5, -5], [5, 5]]	# clusters = [np.random.randn(100, 2) + true_centroid for true_centroid in true_centroids]	# data = np.vstack(clusters)	# print(kmeans(data, len(true_centroids)))		save_history()
2024-11-17-16-09-23 105 59 import numpy as np	import matplotlib.pyplot as plt		# Step 1: Generate 5-dimensional data with overlapping clusters	# We create 3 clusters with significant overlap	np.random.seed(42)	num_clusters = 3	num_dimensions = 5	num_points = np.array([100, 100, 100])	cluster_std_devs = np.array([1.5, 2.5, 2.5])  # High std dev for overlap	center_std_dev = 5.0  # Spread out the cluster centers more		# Generate the dataset	data, labels = create_dataset(num_clusters, num_dimensions, num_points, cluster_std_devs, center_std_dev)		# Step 2: Introduce missing values into the dataset (10% of the data)	nan_data = data.copy()	num_elements = nan_data.size	num_nan = int(0.1 * num_elements)  # 10% of the data	nan_indices = np.random.choice(num_elements, num_nan, replace=False)	nan_data.ravel()[nan_indices] = np.nan		# Step 3: Impute missing values using low-rank SVD with rank 4	imputed_data = impute_missing_values(nan_data, rank=4)		# Step 4: Apply ISOMAP to reduce to 2D for visualization	n_components = 2	data_2d = isomap(imputed_data, n_components=n_components, k=5)		# Step 5: Apply kmeans clustering and calculate silhouette scores for each cluster count	n_clusters_range = range(2, 9)	silhouette_scores = []		for n_clusters in n_clusters_range:	    # Apply kmeans clustering to the reduced data	    centroids, cluster_labels = kmeans(data_2d, k=n_clusters, max_iter=100, tol=1e-4)	    print(centroids)	    plt.scatter(*data_2d.T, c=cluster_labels)	    plt.show()	    	    # Calculate the silhouette score for the current number of clusters	    _, mean_silhouette_score = silhouette_score(data_2d, cluster_labels)	    silhouette_scores.append(mean_silhouette_score)		# Step 6: Identify the best number of clusters based on silhouette score	best_n_clusters = n_clusters_range[np.argmax(silhouette_scores)]	best_silhouette_score = max(silhouette_scores)		# Print the results	print(f"Best number of clusters: {best_n_clusters}")	print(f"Best silhouette score: {best_silhouette_score:.3f}")		# Plot silhouette scores vs. number of clusters	plt.figure(figsize=(10, 6))	plt.plot(n_clusters_range, silhouette_scores, marker='o')	plt.title("Silhouette Scores for Different Numbers of Clusters")	plt.xlabel("Number of Clusters")	plt.ylabel("Mean Silhouette Score")	plt.show()
2024-11-17-16-09-23 105 60 import numpy as np	import matplotlib.pyplot as plt		# Step 1: Generate 5-dimensional data with overlapping clusters	# We create 3 clusters with significant overlap	np.random.seed(42)	num_clusters = 3	num_dimensions = 5	num_points = np.array([100, 100, 100])	cluster_std_devs = np.array([1.5, 2.5, 2.5])  # High std dev for overlap	center_std_dev = 5.0  # Spread out the cluster centers more		# Generate the dataset	data, labels = create_dataset(num_clusters, num_dimensions, num_points, cluster_std_devs, center_std_dev)		# Step 2: Introduce missing values into the dataset (10% of the data)	nan_data = data.copy()	num_elements = nan_data.size	num_nan = int(0.1 * num_elements)  # 10% of the data	nan_indices = np.random.choice(num_elements, num_nan, replace=False)	nan_data.ravel()[nan_indices] = np.nan		# Step 3: Impute missing values using low-rank SVD with rank 4	imputed_data = impute_missing_values(nan_data, rank=4)		# Step 4: Apply ISOMAP to reduce to 2D for visualization	n_components = 2	data_2d = isomap(imputed_data, n_components=n_components, k=5)		# Step 5: Apply kmeans clustering and calculate silhouette scores for each cluster count	n_clusters_range = range(2, 9)	silhouette_scores = []		for n_clusters in n_clusters_range:	    # Apply kmeans clustering to the reduced data	    centroids, cluster_labels = kmeans(data_2d, k=n_clusters, max_iter=100, tol=1e-4)	    print(centroids)	    plt.scatter(*data_2d.T, c=cluster_labels)	    plt.show()	    	    # Calculate the silhouette score for the current number of clusters	    _, mean_silhouette_score = silhouette_score(data_2d, cluster_labels)	    silhouette_scores.append(mean_silhouette_score)		# Step 6: Identify the best number of clusters based on silhouette score	best_n_clusters = n_clusters_range[np.argmax(silhouette_scores)]	best_silhouette_score = max(silhouette_scores)		# Print the results	print(f"Best number of clusters: {best_n_clusters}")	print(f"Best silhouette score: {best_silhouette_score:.3f}")		# Plot silhouette scores vs. number of clusters	plt.figure(figsize=(10, 6))	plt.plot(n_clusters_range, silhouette_scores, marker='o')	plt.title("Silhouette Scores for Different Numbers of Clusters")	plt.xlabel("Number of Clusters")	plt.ylabel("Mean Silhouette Score")	plt.show()
2024-11-17-16-09-23 105 61 import numpy as np	import matplotlib.pyplot as plt		# Step 1: Generate 5-dimensional data with overlapping clusters	# We create 3 clusters with significant overlap	np.random.seed(42)	num_clusters = 3	num_dimensions = 5	num_points = np.array([100, 100, 100])	cluster_std_devs = np.array([1.5, 2.5, 2.5])  # High std dev for overlap	center_std_dev = 5.0  # Spread out the cluster centers more		# Generate the dataset	data, labels = create_dataset(num_clusters, num_dimensions, num_points, cluster_std_devs, center_std_dev)		# Step 2: Introduce missing values into the dataset (10% of the data)	nan_data = data.copy()	num_elements = nan_data.size	num_nan = int(0.1 * num_elements)  # 10% of the data	nan_indices = np.random.choice(num_elements, num_nan, replace=False)	nan_data.ravel()[nan_indices] = np.nan		# Step 3: Impute missing values using low-rank SVD with rank 4	imputed_data = impute_missing_values(nan_data, rank=4)		# Step 4: Apply ISOMAP to reduce to 2D for visualization	n_components = 2	data_2d = isomap(imputed_data, n_components=n_components, k=5)		# Step 5: Apply kmeans clustering and calculate silhouette scores for each cluster count	n_clusters_range = range(2, 9)	silhouette_scores = []		for n_clusters in n_clusters_range:	    # Apply kmeans clustering to the reduced data	    centroids, cluster_labels = kmeans(data_2d, k=n_clusters, max_iter=100, tol=1e-4)	    print(centroids)	    plt.scatter(*data_2d.T, c=cluster_labels)	    plt.show()	    	    # Calculate the silhouette score for the current number of clusters	    _, mean_silhouette_score = silhouette_score(data_2d, cluster_labels)	    silhouette_scores.append(mean_silhouette_score)		# Step 6: Identify the best number of clusters based on silhouette score	best_n_clusters = n_clusters_range[np.argmax(silhouette_scores)]	best_silhouette_score = max(silhouette_scores)		# Print the results	print(f"Best number of clusters: {best_n_clusters}")	print(f"Best silhouette score: {best_silhouette_score:.3f}")		# Plot silhouette scores vs. number of clusters	plt.figure(figsize=(10, 6))	plt.plot(n_clusters_range, silhouette_scores, marker='o')	plt.title("Silhouette Scores for Different Numbers of Clusters")	plt.xlabel("Number of Clusters")	plt.ylabel("Mean Silhouette Score")	plt.show()
2024-11-17-16-09-23 105 62 import numpy as np	import matplotlib.pyplot as plt	from sklearn.manifold import Isomap		# Step 1: Generate 5-dimensional data with overlapping clusters	# We create 3 clusters with significant overlap	np.random.seed(42)	num_clusters = 3	num_dimensions = 5	num_points = np.array([100, 100, 100])	cluster_std_devs = np.array([1.5, 2.5, 2.5])  # High std dev for overlap	center_std_dev = 5.0  # Spread out the cluster centers more		# Generate the dataset	data, labels = create_dataset(num_clusters, num_dimensions, num_points, cluster_std_devs, center_std_dev)		# Step 2: Introduce missing values into the dataset (10% of the data)	nan_data = data.copy()	num_elements = nan_data.size	num_nan = int(0.1 * num_elements)  # 10% of the data	nan_indices = np.random.choice(num_elements, num_nan, replace=False)	nan_data.ravel()[nan_indices] = np.nan		# Step 3: Impute missing values using low-rank SVD with rank 4	imputed_data = impute_missing_values(nan_data, rank=4)		# Step 4: Apply ISOMAP to reduce to 2D for visualization	n_components = 2	# data_2d = isomap(imputed_data, n_components=n_components, k=5)	data_2d = Isomap(n_components=2, n_neighbors=5).fit_transform(imputed_data)		# Step 5: Apply kmeans clustering and calculate silhouette scores for each cluster count	n_clusters_range = range(2, 9)	silhouette_scores = []		for n_clusters in n_clusters_range:	    # Apply kmeans clustering to the reduced data	    centroids, cluster_labels = kmeans(data_2d, k=n_clusters, max_iter=100, tol=1e-4)	    print(centroids)	    plt.scatter(*data_2d.T, c=cluster_labels)	    plt.show()	    	    # Calculate the silhouette score for the current number of clusters	    _, mean_silhouette_score = silhouette_score(data_2d, cluster_labels)	    silhouette_scores.append(mean_silhouette_score)		# Step 6: Identify the best number of clusters based on silhouette score	best_n_clusters = n_clusters_range[np.argmax(silhouette_scores)]	best_silhouette_score = max(silhouette_scores)		# Print the results	print(f"Best number of clusters: {best_n_clusters}")	print(f"Best silhouette score: {best_silhouette_score:.3f}")		# Plot silhouette scores vs. number of clusters	plt.figure(figsize=(10, 6))	plt.plot(n_clusters_range, silhouette_scores, marker='o')	plt.title("Silhouette Scores for Different Numbers of Clusters")	plt.xlabel("Number of Clusters")	plt.ylabel("Mean Silhouette Score")	plt.show()
2024-11-17-16-09-23 105 63 import numpy as np	import matplotlib.pyplot as plt	from sklearn.manifold import Isomap		# Step 1: Generate 5-dimensional data with overlapping clusters	# We create 3 clusters with significant overlap	np.random.seed(42)	num_clusters = 3	num_dimensions = 5	num_points = np.array([100, 100, 100])	cluster_std_devs = np.array([1.5, 2.5, 2.5])  # High std dev for overlap	center_std_dev = 5.0  # Spread out the cluster centers more		# Generate the dataset	data, labels = create_dataset(num_clusters, num_dimensions, num_points, cluster_std_devs, center_std_dev)		# Step 2: Introduce missing values into the dataset (10% of the data)	nan_data = data.copy()	num_elements = nan_data.size	num_nan = int(0.1 * num_elements)  # 10% of the data	nan_indices = np.random.choice(num_elements, num_nan, replace=False)	nan_data.ravel()[nan_indices] = np.nan		# Step 3: Impute missing values using low-rank SVD with rank 4	imputed_data = impute_missing_values(nan_data, rank=4)		# Step 4: Apply ISOMAP to reduce to 2D for visualization	n_components = 2	# data_2d = isomap(imputed_data, n_components=n_components, k=5)	data_2d = Isomap(n_components=2, n_neighbors=5).fit_transform(imputed_data)		# Step 5: Apply kmeans clustering and calculate silhouette scores for each cluster count	n_clusters_range = range(2, 9)	silhouette_scores = []		for n_clusters in n_clusters_range:	    # Apply kmeans clustering to the reduced data	    centroids, cluster_labels = kmeans(data_2d, k=n_clusters, max_iter=100, tol=1e-4)	    print(centroids)	    plt.scatter(*data_2d.T, c=cluster_labels)	    plt.show()	    	    # Calculate the silhouette score for the current number of clusters	    _, mean_silhouette_score = silhouette_score(data_2d, cluster_labels)	    silhouette_scores.append(mean_silhouette_score)		# Step 6: Identify the best number of clusters based on silhouette score	best_n_clusters = n_clusters_range[np.argmax(silhouette_scores)]	best_silhouette_score = max(silhouette_scores)		# Print the results	print(f"Best number of clusters: {best_n_clusters}")	print(f"Best silhouette score: {best_silhouette_score:.3f}")		# Plot silhouette scores vs. number of clusters	plt.figure(figsize=(10, 6))	plt.plot(n_clusters_range, silhouette_scores, marker='o')	plt.title("Silhouette Scores for Different Numbers of Clusters")	plt.xlabel("Number of Clusters")	plt.ylabel("Mean Silhouette Score")	plt.show()
2024-11-17-16-09-23 105 64 def silhouette_score(data: np.ndarray, labels: np.ndarray) -> Tuple[np.ndarray, float]:	    num_samples = len(labels)	    scores = np.zeros(num_samples)	    a_of_is = within_cluster_distances(data, labels)	    b_of_is = nearest_cluster_distances(data, labels)	    for i in range(num_samples):	        a_of_i = a_of_is[i]	        b_of_i = b_of_is[i]	        score = (b_of_i - a_of_i) / max(a_of_i, b_of_i)	        scores[i] = score	    mean_score = np.mean(scores)	    return scores, mean_score		data, labels = create_dataset(4, 5, np.array([4, 4, 4, 4]), np.array([1, 1, 1, 1]), 10)	print(silhouette_score(data, labels))		save_history()
2024-11-17-16-09-23 105 65 def silhouette_score(data: np.ndarray, labels: np.ndarray) -> Tuple[np.ndarray, float]:	    num_samples = len(labels)	    scores = np.zeros(num_samples)	    a_of_is = within_cluster_distances(data, labels)	    b_of_is = nearest_cluster_distances(data, labels)	    for i in range(num_samples):	        a_of_i = a_of_is[i]	        b_of_i = b_of_is[i]	        score = (b_of_i - a_of_i) / max(a_of_i, b_of_i)	        scores[i] = score	    mean_score = np.mean(scores)	    return scores, mean_score		data, labels = create_dataset(4, 5, np.array([100, 100, 100, 100]), np.array([1, 1, 1, 1]), 10)	print(silhouette_score(data, labels))		save_history()
2024-11-17-16-09-23 105 66 def silhouette_score(data: np.ndarray, labels: np.ndarray) -> Tuple[np.ndarray, float]:	    num_samples = len(labels)	    scores = np.zeros(num_samples)	    a_of_is = within_cluster_distances(data, labels)	    b_of_is = nearest_cluster_distances(data, labels)	    for i in range(num_samples):	        a_of_i = a_of_is[i]	        b_of_i = b_of_is[i]	        score = (b_of_i - a_of_i) / max(a_of_i, b_of_i)	        scores[i] = score	    mean_score = np.mean(scores)	    return scores, mean_score		data, labels = create_dataset(4, 5, np.array([4, 4, 4, 4]), np.array([10, 1, 1, 1]), 10)	print(silhouette_score(data, labels))		save_history()
2024-11-17-16-09-23 105 67 def silhouette_score(data: np.ndarray, labels: np.ndarray) -> Tuple[np.ndarray, float]:	    num_samples = len(labels)	    scores = np.zeros(num_samples)	    a_of_is = within_cluster_distances(data, labels)	    b_of_is = nearest_cluster_distances(data, labels)	    for i in range(num_samples):	        a_of_i = a_of_is[i]	        b_of_i = b_of_is[i]	        score = (b_of_i - a_of_i) / max(a_of_i, b_of_i)	        scores[i] = score	    mean_score = np.mean(scores)	    return scores, mean_score		data, labels = create_dataset(4, 5, np.array([4, 4, 4, 4]), np.array([10, 1, 5, 3]), 10)	print(silhouette_score(data, labels))		save_history()
2024-11-17-16-09-23 105 68 def silhouette_score(data: np.ndarray, labels: np.ndarray) -> Tuple[np.ndarray, float]:	    num_samples = len(labels)	    scores = np.zeros(num_samples)	    a_of_is = within_cluster_distances(data, labels)	    b_of_is = nearest_cluster_distances(data, labels)	    for i in range(num_samples):	        a_of_i = a_of_is[i]	        b_of_i = b_of_is[i]	        score = (b_of_i - a_of_i) / max(a_of_i, b_of_i)	        scores[i] = score	    mean_score = np.mean(scores)	    return scores, mean_score		data, labels = create_dataset(4, 5, np.array([4, 4, 4, 4]), np.array([1, 3, 6, 10]), 10)	print(silhouette_score(data, labels))		save_history()
2024-11-17-16-09-23 105 69 def silhouette_score(data: np.ndarray, labels: np.ndarray) -> Tuple[np.ndarray, float]:	    num_samples = len(labels)	    scores = np.zeros(num_samples)	    a_of_is = within_cluster_distances(data, labels)	    b_of_is = nearest_cluster_distances(data, labels)	    for i in range(num_samples):	        a_of_i = a_of_is[i]	        b_of_i = b_of_is[i]	        score = (b_of_i - a_of_i) / max(a_of_i, b_of_i)	        scores[i] = score	    mean_score = np.mean(scores)	    return scores, mean_score		data, labels = create_dataset(4, 5, np.array([4, 4, 4, 4]), np.array([1, 3, 6, 10]), 10)	plot_dataset(data, labels)	print(silhouette_score(data, labels))		save_history()
2024-11-17-16-09-23 105 70 def silhouette_score(data: np.ndarray, labels: np.ndarray) -> Tuple[np.ndarray, float]:	    num_samples = len(labels)	    scores = np.zeros(num_samples)	    a_of_is = within_cluster_distances(data, labels)	    b_of_is = nearest_cluster_distances(data, labels)	    for i in range(num_samples):	        a_of_i = a_of_is[i]	        b_of_i = b_of_is[i]	        score = (b_of_i - a_of_i) / max(a_of_i, b_of_i)	        scores[i] = score	    mean_score = np.mean(scores)	    return scores, mean_score		data, labels = create_dataset(4, 5, np.array([4, 4, 4, 4]), np.array([1, 3, 6, 6]), 10)	plot_dataset(data, labels)	print(silhouette_score(data, labels))		save_history()
2024-11-17-16-09-23 105 71 def silhouette_score(data: np.ndarray, labels: np.ndarray) -> Tuple[np.ndarray, float]:	    num_samples = len(labels)	    scores = np.zeros(num_samples)	    a_of_is = within_cluster_distances(data, labels)	    b_of_is = nearest_cluster_distances(data, labels)	    for i in range(num_samples):	        a_of_i = a_of_is[i]	        b_of_i = b_of_is[i]	        score = (b_of_i - a_of_i) / max(a_of_i, b_of_i)	        scores[i] = score	    mean_score = np.mean(scores)	    return scores, mean_score		data, labels = create_dataset(4, 5, np.array([4, 4, 4, 4]), np.array([1, 3, 6, 6]), 10)	plot_dataset(data, labels)	print(silhouette_score(data, labels))		save_history()
2024-11-17-16-09-23 105 72 def silhouette_score(data: np.ndarray, labels: np.ndarray) -> Tuple[np.ndarray, float]:	    num_samples = len(labels)	    scores = np.zeros(num_samples)	    a_of_is = within_cluster_distances(data, labels)	    b_of_is = nearest_cluster_distances(data, labels)	    for i in range(num_samples):	        a_of_i = a_of_is[i]	        b_of_i = b_of_is[i]	        score = (b_of_i - a_of_i) / max(a_of_i, b_of_i)	        scores[i] = score	    mean_score = np.mean(scores)	    return scores, mean_score		data, labels = create_dataset(4, 5, np.array([4, 4, 4, 4]), np.array([1, 3, 6, 6]), 10)	plot_dataset(data, labels)	print(silhouette_score(data, labels))		save_history()
2024-11-17-16-09-23 105 73 def silhouette_score(data: np.ndarray, labels: np.ndarray) -> Tuple[np.ndarray, float]:	    num_samples = len(labels)	    scores = np.zeros(num_samples)	    a_of_is = within_cluster_distances(data, labels)	    b_of_is = nearest_cluster_distances(data, labels)	    for i in range(num_samples):	        a_of_i = a_of_is[i]	        b_of_i = b_of_is[i]	        score = (b_of_i - a_of_i) / max(a_of_i, b_of_i)	        scores[i] = score	    mean_score = np.mean(scores)	    return scores, mean_score		data, labels = create_dataset(4, 5, np.array([4, 4, 4, 4]), np.array([1, 3, 6, 6]), 10)	plot_dataset(data, labels)	print(silhouette_score(data, labels))		save_history()
2024-11-17-16-09-23 105 74 import numpy as np	import matplotlib.pyplot as plt	from sklearn.manifold import Isomap		# Step 1: Generate 5-dimensional data with overlapping clusters	# We create 3 clusters with significant overlap	np.random.seed(42)	num_clusters = 3	num_dimensions = 5	num_points = np.array([100, 100, 100])	cluster_std_devs = np.array([1.5, 2.5, 2.5])  # High std dev for overlap	center_std_dev = 5.0  # Spread out the cluster centers more		# Generate the dataset	data, labels = create_dataset(num_clusters, num_dimensions, num_points, cluster_std_devs, center_std_dev)		# Step 2: Introduce missing values into the dataset (10% of the data)	nan_data = data.copy()	num_elements = nan_data.size	num_nan = int(0.1 * num_elements)  # 10% of the data	nan_indices = np.random.choice(num_elements, num_nan, replace=False)	nan_data.ravel()[nan_indices] = np.nan		# Step 3: Impute missing values using low-rank SVD with rank 4	imputed_data = impute_missing_values(nan_data, rank=4)		# Step 4: Apply ISOMAP to reduce to 2D for visualization	n_components = 2	data_2d = isomap(imputed_data, n_components=n_components, k=5)	# data_2d = Isomap(n_components=2, n_neighbors=5).fit_transform(imputed_data)		# Step 5: Apply kmeans clustering and calculate silhouette scores for each cluster count	n_clusters_range = range(2, 9)	silhouette_scores = []		for n_clusters in n_clusters_range:	    # Apply kmeans clustering to the reduced data	    centroids, cluster_labels = kmeans(data_2d, k=n_clusters, max_iter=100, tol=1e-4)	    print(centroids)	    plt.scatter(*data_2d.T, c=cluster_labels)	    plt.show()	    	    # Calculate the silhouette score for the current number of clusters	    _, mean_silhouette_score = silhouette_score(data_2d, cluster_labels)	    silhouette_scores.append(mean_silhouette_score)		# Step 6: Identify the best number of clusters based on silhouette score	best_n_clusters = n_clusters_range[np.argmax(silhouette_scores)]	best_silhouette_score = max(silhouette_scores)		# Print the results	print(f"Best number of clusters: {best_n_clusters}")	print(f"Best silhouette score: {best_silhouette_score:.3f}")		# Plot silhouette scores vs. number of clusters	plt.figure(figsize=(10, 6))	plt.plot(n_clusters_range, silhouette_scores, marker='o')	plt.title("Silhouette Scores for Different Numbers of Clusters")	plt.xlabel("Number of Clusters")	plt.ylabel("Mean Silhouette Score")	plt.show()
2024-11-17-16-09-23 105 75 from sklearn.manifold import MDS		def apply_mds(geodesic_distances: np.ndarray, n_components: int) -> np.ndarray:	    embedding = MDS(n_components=n_components, dissimilarity='euclidean')	    geodesic_distances[geodesic_distances == np.inf] = 1e9	    reduced_data = embedding.fit_transform(geodesic_distances)	    return reduced_data		save_history()
2024-11-17-16-09-23 105 76 import numpy as np	import matplotlib.pyplot as plt	from sklearn.manifold import Isomap		# Step 1: Generate 5-dimensional data with overlapping clusters	# We create 3 clusters with significant overlap	np.random.seed(42)	num_clusters = 3	num_dimensions = 5	num_points = np.array([100, 100, 100])	cluster_std_devs = np.array([1.5, 2.5, 2.5])  # High std dev for overlap	center_std_dev = 5.0  # Spread out the cluster centers more		# Generate the dataset	data, labels = create_dataset(num_clusters, num_dimensions, num_points, cluster_std_devs, center_std_dev)		# Step 2: Introduce missing values into the dataset (10% of the data)	nan_data = data.copy()	num_elements = nan_data.size	num_nan = int(0.1 * num_elements)  # 10% of the data	nan_indices = np.random.choice(num_elements, num_nan, replace=False)	nan_data.ravel()[nan_indices] = np.nan		# Step 3: Impute missing values using low-rank SVD with rank 4	imputed_data = impute_missing_values(nan_data, rank=4)		# Step 4: Apply ISOMAP to reduce to 2D for visualization	n_components = 2	data_2d = isomap(imputed_data, n_components=n_components, k=5)	# data_2d = Isomap(n_components=2, n_neighbors=5).fit_transform(imputed_data)		# Step 5: Apply kmeans clustering and calculate silhouette scores for each cluster count	n_clusters_range = range(2, 9)	silhouette_scores = []		for n_clusters in n_clusters_range:	    # Apply kmeans clustering to the reduced data	    centroids, cluster_labels = kmeans(data_2d, k=n_clusters, max_iter=100, tol=1e-4)	    print(centroids)	    plt.scatter(*data_2d.T, c=cluster_labels)	    plt.show()	    	    # Calculate the silhouette score for the current number of clusters	    _, mean_silhouette_score = silhouette_score(data_2d, cluster_labels)	    silhouette_scores.append(mean_silhouette_score)		# Step 6: Identify the best number of clusters based on silhouette score	best_n_clusters = n_clusters_range[np.argmax(silhouette_scores)]	best_silhouette_score = max(silhouette_scores)		# Print the results	print(f"Best number of clusters: {best_n_clusters}")	print(f"Best silhouette score: {best_silhouette_score:.3f}")		# Plot silhouette scores vs. number of clusters	plt.figure(figsize=(10, 6))	plt.plot(n_clusters_range, silhouette_scores, marker='o')	plt.title("Silhouette Scores for Different Numbers of Clusters")	plt.xlabel("Number of Clusters")	plt.ylabel("Mean Silhouette Score")	plt.show()
2024-11-17-16-09-23 105 77 from sklearn.manifold import MDS		def apply_mds(geodesic_distances: np.ndarray, n_components: int) -> np.ndarray:	    embedding = MDS(n_components=n_components, dissimilarity='precomputed')	    geodesic_distances[geodesic_distances == np.inf] = 1e9	    reduced_data = embedding.fit_transform(geodesic_distances)	    return reduced_data		save_history()
2024-11-17-16-09-23 105 78 from sklearn.neighbors import NearestNeighbors		def construct_knn_graph(data: np.ndarray, k: int) -> np.ndarray:	    knn = NearestNeighbors(n_neighbors=k)	    knn.fit(data)	    distances, indices = knn.kneighbors(data)	    num_samples = data.shape[0]	    knn_graph = np.zeros((num_samples, num_samples))	    for i, (distances_vector, indices_vector) in enumerate(zip(distances, indices)):	        for distance, j in zip(distances_vector, indices_vector):	            knn_graph[i, j] = distance	    return knn_graph		# X = [[0], [3], [1]]	# k = 2	# neigh = NearestNeighbors(n_neighbors=k)	# neigh.fit(X)	# print(neigh.kneighbors_graph(X, mode='distance').toarray())	# print(construct_knn_graph(np.array(X), k))		save_history()
2024-11-17-16-09-23 105 79 from scipy.sparse import csr_matrix	from scipy.sparse.csgraph import dijkstra		def compute_geodesic_distances(knn_graph: np.ndarray) -> np.ndarray:	    num_samples = knn_graph.shape[0]	    geodesic_distances = np.zeros((num_samples, num_samples))	    knn_cs_graph = csr_matrix(knn_graph)	    for i in range(num_samples):	        distances_vector = dijkstra(csgraph=knn_cs_graph, directed=True, indices=i)	        geodesic_distances[i] = distances_vector	    return geodesic_distances		# graph = np.array([	#     [0, 1, 2, 0],	#     [0, 0, 0, 1],	#     [0, 0, 0, 3],	#     [0, 0, 0, 0]	#     ])	# print(compute_geodesic_distances(graph))	# print(dijkstra(csgraph=csr_matrix(graph)))		save_history()
2024-11-17-16-09-23 105 80 import numpy as np	import matplotlib.pyplot as plt	from sklearn.manifold import Isomap		# Step 1: Generate 5-dimensional data with overlapping clusters	# We create 3 clusters with significant overlap	np.random.seed(42)	num_clusters = 3	num_dimensions = 5	num_points = np.array([100, 100, 100])	cluster_std_devs = np.array([1.5, 2.5, 2.5])  # High std dev for overlap	center_std_dev = 5.0  # Spread out the cluster centers more		# Generate the dataset	data, labels = create_dataset(num_clusters, num_dimensions, num_points, cluster_std_devs, center_std_dev)		# Step 2: Introduce missing values into the dataset (10% of the data)	nan_data = data.copy()	num_elements = nan_data.size	num_nan = int(0.1 * num_elements)  # 10% of the data	nan_indices = np.random.choice(num_elements, num_nan, replace=False)	nan_data.ravel()[nan_indices] = np.nan		# Step 3: Impute missing values using low-rank SVD with rank 4	imputed_data = impute_missing_values(nan_data, rank=4)		# Step 4: Apply ISOMAP to reduce to 2D for visualization	n_components = 2	data_2d = isomap(imputed_data, n_components=n_components, k=5)	# data_2d = Isomap(n_components=2, n_neighbors=5).fit_transform(imputed_data)		# Step 5: Apply kmeans clustering and calculate silhouette scores for each cluster count	n_clusters_range = range(2, 9)	silhouette_scores = []		for n_clusters in n_clusters_range:	    # Apply kmeans clustering to the reduced data	    centroids, cluster_labels = kmeans(data_2d, k=n_clusters, max_iter=100, tol=1e-4)	    print(centroids)	    plt.scatter(*data_2d.T, c=cluster_labels)	    plt.show()	    	    # Calculate the silhouette score for the current number of clusters	    _, mean_silhouette_score = silhouette_score(data_2d, cluster_labels)	    silhouette_scores.append(mean_silhouette_score)		# Step 6: Identify the best number of clusters based on silhouette score	best_n_clusters = n_clusters_range[np.argmax(silhouette_scores)]	best_silhouette_score = max(silhouette_scores)		# Print the results	print(f"Best number of clusters: {best_n_clusters}")	print(f"Best silhouette score: {best_silhouette_score:.3f}")		# Plot silhouette scores vs. number of clusters	plt.figure(figsize=(10, 6))	plt.plot(n_clusters_range, silhouette_scores, marker='o')	plt.title("Silhouette Scores for Different Numbers of Clusters")	plt.xlabel("Number of Clusters")	plt.ylabel("Mean Silhouette Score")	plt.show()
2024-11-17-16-09-23 105 81 from sklearn.neighbors import NearestNeighbors		def construct_knn_graph(data: np.ndarray, k: int) -> np.ndarray:	    knn = NearestNeighbors(n_neighbors=k)	    knn.fit(data)	    distances, indices = knn.kneighbors(data)	    num_samples = data.shape[0]	    knn_graph = np.zeros((num_samples, num_samples))	    for i, (distances_vector, indices_vector) in enumerate(zip(distances, indices)):	        for distance, j in zip(distances_vector, indices_vector):	            knn_graph[i, j] = distance	            knn_graph[j, i] = distance	    return knn_graph		# X = [[0], [3], [1]]	# k = 2	# neigh = NearestNeighbors(n_neighbors=k)	# neigh.fit(X)	# print(neigh.kneighbors_graph(X, mode='distance').toarray())	# print(construct_knn_graph(np.array(X), k))		save_history()
2024-11-17-16-09-23 105 82 from scipy.sparse import csr_matrix	from scipy.sparse.csgraph import dijkstra		def compute_geodesic_distances(knn_graph: np.ndarray) -> np.ndarray:	    num_samples = knn_graph.shape[0]	    geodesic_distances = np.zeros((num_samples, num_samples))	    knn_cs_graph = csr_matrix(knn_graph)	    for i in range(num_samples):	        distances_vector = dijkstra(csgraph=knn_cs_graph, directed=False, indices=i)	        geodesic_distances[i] = distances_vector	    return geodesic_distances		# graph = np.array([	#     [0, 1, 2, 0],	#     [0, 0, 0, 1],	#     [0, 0, 0, 3],	#     [0, 0, 0, 0]	#     ])	# print(compute_geodesic_distances(graph))	# print(dijkstra(csgraph=csr_matrix(graph)))		save_history()
2024-11-17-16-09-23 105 83 import numpy as np	import matplotlib.pyplot as plt	from sklearn.manifold import Isomap		# Step 1: Generate 5-dimensional data with overlapping clusters	# We create 3 clusters with significant overlap	np.random.seed(42)	num_clusters = 3	num_dimensions = 5	num_points = np.array([100, 100, 100])	cluster_std_devs = np.array([1.5, 2.5, 2.5])  # High std dev for overlap	center_std_dev = 5.0  # Spread out the cluster centers more		# Generate the dataset	data, labels = create_dataset(num_clusters, num_dimensions, num_points, cluster_std_devs, center_std_dev)		# Step 2: Introduce missing values into the dataset (10% of the data)	nan_data = data.copy()	num_elements = nan_data.size	num_nan = int(0.1 * num_elements)  # 10% of the data	nan_indices = np.random.choice(num_elements, num_nan, replace=False)	nan_data.ravel()[nan_indices] = np.nan		# Step 3: Impute missing values using low-rank SVD with rank 4	imputed_data = impute_missing_values(nan_data, rank=4)		# Step 4: Apply ISOMAP to reduce to 2D for visualization	n_components = 2	data_2d = isomap(imputed_data, n_components=n_components, k=5)	# data_2d = Isomap(n_components=2, n_neighbors=5).fit_transform(imputed_data)		# Step 5: Apply kmeans clustering and calculate silhouette scores for each cluster count	n_clusters_range = range(2, 9)	silhouette_scores = []		for n_clusters in n_clusters_range:	    # Apply kmeans clustering to the reduced data	    centroids, cluster_labels = kmeans(data_2d, k=n_clusters, max_iter=100, tol=1e-4)	    print(centroids)	    plt.scatter(*data_2d.T, c=cluster_labels)	    plt.show()	    	    # Calculate the silhouette score for the current number of clusters	    _, mean_silhouette_score = silhouette_score(data_2d, cluster_labels)	    silhouette_scores.append(mean_silhouette_score)		# Step 6: Identify the best number of clusters based on silhouette score	best_n_clusters = n_clusters_range[np.argmax(silhouette_scores)]	best_silhouette_score = max(silhouette_scores)		# Print the results	print(f"Best number of clusters: {best_n_clusters}")	print(f"Best silhouette score: {best_silhouette_score:.3f}")		# Plot silhouette scores vs. number of clusters	plt.figure(figsize=(10, 6))	plt.plot(n_clusters_range, silhouette_scores, marker='o')	plt.title("Silhouette Scores for Different Numbers of Clusters")	plt.xlabel("Number of Clusters")	plt.ylabel("Mean Silhouette Score")	plt.show()
2024-11-17-16-09-23 105 84 from sklearn.neighbors import NearestNeighbors		def construct_knn_graph(data: np.ndarray, k: int) -> np.ndarray:	    knn = NearestNeighbors(n_neighbors=k)	    knn.fit(data)	    distances, indices = knn.kneighbors(data)	    num_samples = data.shape[0]	    knn_graph = np.zeros((num_samples, num_samples))	    for i, (distances_vector, indices_vector) in enumerate(zip(distances, indices)):	        for distance, j in zip(distances_vector, indices_vector):	            knn_graph[i, j] = distance	            knn_graph[j, i] = distance	    return knn_graph		X = [[0], [3], [1]]	k = 2	neigh = NearestNeighbors(n_neighbors=k)	neigh.fit(X)	print(neigh.kneighbors_graph(X, mode='distance').toarray())	print(construct_knn_graph(np.array(X), k))		save_history()
2024-11-17-16-09-23 105 85 from sklearn.neighbors import NearestNeighbors		def construct_knn_graph(data: np.ndarray, k: int) -> np.ndarray:	    knn = NearestNeighbors(n_neighbors=k)	    knn.fit(data)	    distances, indices = knn.kneighbors(data)	    num_samples = data.shape[0]	    knn_graph = np.zeros((num_samples, num_samples))	    for i, (distances_vector, indices_vector) in enumerate(zip(distances, indices)):	        for distance, j in zip(distances_vector, indices_vector):	            knn_graph[i, j] = distance	            knn_graph[j, i] = distance	    return knn_graph		X = [[0], [3], [1]]	k = 2	neigh = NearestNeighbors(n_neighbors=k + 1)	neigh.fit(X)	print(neigh.kneighbors_graph(X, mode='distance').toarray())	print(construct_knn_graph(np.array(X), k + 1))		save_history()
2024-11-17-16-09-23 105 86 from sklearn.neighbors import NearestNeighbors		def construct_knn_graph(data: np.ndarray, k: int) -> np.ndarray:	    knn = NearestNeighbors(n_neighbors=k + 1)	    knn.fit(data)	    distances, indices = knn.kneighbors(data)	    num_samples = data.shape[0]	    knn_graph = np.zeros((num_samples, num_samples))	    for i, (distances_vector, indices_vector) in enumerate(zip(distances, indices)):	        for distance, j in zip(distances_vector, indices_vector):	            knn_graph[i, j] = distance	            knn_graph[j, i] = distance	    return knn_graph		X = [[0], [3], [1]]	k = 2	neigh = NearestNeighbors(n_neighbors=k + 1)	neigh.fit(X)	print(neigh.kneighbors_graph(X, mode='distance').toarray())	print(construct_knn_graph(np.array(X), k))		save_history()
