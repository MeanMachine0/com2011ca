2024-11-17-14-12-37 104 1 get_ipython().run_line_magic('matplotlib', 'inline')	import matplotlib.pyplot as plt	from typing import Tuple	import numpy as np	import scipy as sp	from submission_utils import save_history, check_and_prepare_for_submission	# import warnings filter	from warnings import simplefilter	# ignore all future warnings	simplefilter(action='ignore', category=FutureWarning)
2024-11-17-14-12-37 104 2 def create_dataset(num_clusters: int, 	                   num_dimensions: int, 	                   num_points: np.ndarray, 	                   cluster_std_devs: np.ndarray, 	                   center_std_dev: float) -> Tuple[np.ndarray, np.ndarray]:	    data = []	    labels = []	    origin = np.zeros((num_dimensions,))	    cluster_centres = np.random.normal(loc=origin, scale=center_std_dev, size=(num_clusters, num_dimensions))	    for i in range(num_clusters):	        cluster_points = np.random.normal(loc=cluster_centres[i], scale=cluster_std_devs[i], size=(num_points[i], num_dimensions))	        for point in cluster_points:	            data.append(point)	            labels.append(i)	    data = np.array(data)	    labels = np.array(labels)	    return (data, labels)	# data, labels = create_dataset(4, 5, np.array([4, 4, 4, 4]), np.array([1, 3, 5, 7, 9]), 10)		save_history()
2024-11-17-14-12-37 104 3 # from sklearn.decomposition import PCA	# from sklearn.datasets import load_iris		def pca_with_svd(data: np.ndarray, n_components: int) -> np.ndarray:	    centred_data = data - np.mean(data, axis=0)	    u, s, vh = np.linalg.svd(centred_data)	    transformed_data = centred_data.dot(vh.T[:, :n_components])	    return transformed_data		# df = load_iris()	# X = df.data	# pca = PCA(n_components=2)	# X_proj = pca.fit_transform(X)	# print(X_proj)	# Y = df.data	# print(pca_with_svd(Y, 2))	save_history()
2024-11-17-14-12-37 104 4 import matplotlib.pyplot as plt	from sklearn.decomposition import PCA		def plot_dataset(data: np.ndarray, labels: np.ndarray) -> None:	    if (data.ndim == 1):	        return	    data_2d = PCA(n_components=2).fit_transform(data)	    plt.scatter(*data_2d.T, c=labels)	    plt.axis('equal')	    plt.xlabel('First PCA component')	    plt.ylabel('Second PCA component')	    plt.grid()	    plt.show()	    return		save_history()
2024-11-17-14-12-37 104 5 def impute_missing_values(data: np.ndarray, rank: int) -> np.ndarray:	    no_nan_data = np.nan_to_num(data, nan=0)	    u, s, vh = np.linalg.svd(no_nan_data)	    s = np.diag(s)	    num_cols_u = u.shape[1]	    if num_cols_u > rank:	        u_trunc = u[:, :rank]	    else:	        u_trunc = u	    num_cols_vh = vh.shape[1]	    if num_cols_vh > rank:	        vh_trunc = vh[:rank, :]	    else:	        vh_trunc = vh	    num_sinuglar_values = s.shape[0]	    if num_sinuglar_values > rank:	        s_trunc = s[:rank, :rank]	    else:	        s_trunc = s	    trunc_svd_data = np.dot(np.dot(u_trunc, s_trunc), vh_trunc)	    nan_mask = np.isnan(data)	    nan_indices = np.where(nan_mask)	    imputed_data = data.copy()	    for row_ind, col_ind in zip(nan_indices[0], nan_indices[1]):	        impute_value = trunc_svd_data[row_ind, col_ind]	        imputed_data[row_ind, col_ind] = impute_value	    return imputed_data		save_history()
2024-11-17-14-12-37 104 6 # Step 1: Create a 5-dimensional dataset with 3 clusters	num_clusters = 3	num_dimensions = 5	num_points = np.array([100, 100, 100])	cluster_std_devs = np.array([1.0, 1.0, 1.0])	center_std_dev = 3.0		data, labels = create_dataset(num_clusters, num_dimensions, num_points, cluster_std_devs, center_std_dev)		# Step 2: Plot the original data (2D projection using PCA)	print("Original Data (2D projection)")	plot_dataset(data, labels)		# Step 3: Introduce 10% missing values	nan_data = data.copy()	num_elements = nan_data.size	num_nan = int(0.1 * num_elements)  # 10% of the data		# Randomly choose 10% of indices to set as NaN	nan_indices = np.random.choice(num_elements, num_nan, replace=False)	nan_data.ravel()[nan_indices] = np.nan		# Step 4: Impute missing values using rank-4 SVD approximation	imputed_data = impute_missing_values(nan_data, rank=4)		# Step 5: Plot the imputed data (2D projection using PCA)	print("Imputed Data (2D projection)")	plot_dataset(imputed_data, labels)
2024-11-17-14-12-37 104 7 import matplotlib.pyplot as plt	from sklearn.decomposition import PCA		def plot_dataset(data: np.ndarray, labels: np.ndarray) -> None:	    if (data.ndim == 1):	        return	    data_2d = PCA(n_components=2).fit_transform(data)	    plt.scatter(*data_2d.T, c=labels, alpha=0.5)	    plt.axis('equal')	    plt.xlabel('First PCA component')	    plt.ylabel('Second PCA component')	    plt.grid()	    plt.show()	    return		save_history()
2024-11-17-14-12-37 104 8 # Step 1: Create a 5-dimensional dataset with 3 clusters	num_clusters = 3	num_dimensions = 5	num_points = np.array([100, 100, 100])	cluster_std_devs = np.array([1.0, 1.0, 1.0])	center_std_dev = 3.0		data, labels = create_dataset(num_clusters, num_dimensions, num_points, cluster_std_devs, center_std_dev)		# Step 2: Plot the original data (2D projection using PCA)	print("Original Data (2D projection)")	plot_dataset(data, labels)		# Step 3: Introduce 10% missing values	nan_data = data.copy()	num_elements = nan_data.size	num_nan = int(0.1 * num_elements)  # 10% of the data		# Randomly choose 10% of indices to set as NaN	nan_indices = np.random.choice(num_elements, num_nan, replace=False)	nan_data.ravel()[nan_indices] = np.nan		# Step 4: Impute missing values using rank-4 SVD approximation	imputed_data = impute_missing_values(nan_data, rank=4)		# Step 5: Plot the imputed data (2D projection using PCA)	print("Imputed Data (2D projection)")	plot_dataset(imputed_data, labels)
2024-11-17-14-12-37 104 9 from sklearn.neighbors import NearestNeighbors		def construct_knn_graph(data: np.ndarray, k: int) -> np.ndarray:	    knn = NearestNeighbors(n_neighbors=k)	    knn.fit(data)	    distances, indices = knn.kneighbors(data)	    num_samples = data.shape[0]	    knn_graph = np.zeros((num_samples, num_samples))	    for i, (distances_vector, indices_vector) in enumerate(zip(distances, indices)):	        for distance, j in zip(distances_vector, indices_vector):	            knn_graph[i, j] = distance	    return knn_graph		X = [[0], [3], [1]]	k = 2	neigh = NearestNeighbors(n_neighbors=k)	neigh.fit(X)	print(neigh.kneighbors_graph(X, mode='distance').toarray())	print(construct_knn_graph(np.array(X), k))		save_history()
2024-11-17-14-12-37 104 10 from sklearn.neighbors import NearestNeighbors		def construct_knn_graph(data: np.ndarray, k: int) -> np.ndarray:	    knn = NearestNeighbors(n_neighbors=k)	    knn.fit(data)	    distances, indices = knn.kneighbors(data)	    num_samples = data.shape[0]	    knn_graph = np.zeros((num_samples, num_samples))	    for i, (distances_vector, indices_vector) in enumerate(zip(distances, indices)):	        for distance, j in zip(distances_vector, indices_vector):	            knn_graph[i, j] = distance	    print(knn_graph)	    return knn_graph		X = [[0], [3], [1]]	k = 2	neigh = NearestNeighbors(n_neighbors=k)	neigh.fit(X)	print(neigh.kneighbors_graph(X, mode='distance').toarray())	print(construct_knn_graph(np.array(X), k))		save_history()
2024-11-17-14-12-37 104 11 from scipy.sparse import csr_matrix	from scipy.sparse.csgraph import dijkstra		def compute_geodesic_distances(knn_graph: np.ndarray) -> np.ndarray:	    num_samples = knn_graph.shape[0]	    geodesic_distances = np.zeros((num_samples, num_samples))	    knn_cs_graph = csr_matrix(knn_graph)	    for i in range(num_samples):	        distances_vector = dijkstra(csgraph=knn_cs_graph, indices=i)	        geodesic_distances[i] = distances_vector	    print(geodesic_distances)	    return geodesic_distances		# graph = np.array([	#     [0, 1, 2, 0],	#     [0, 0, 0, 1],	#     [0, 0, 0, 3],	#     [0, 0, 0, 0]	#     ])	# print(compute_geodesic_distances(graph))	# print(dijkstra(csgraph=csr_matrix(graph), directed=False))		save_history()
2024-11-17-14-12-37 104 12 from sklearn.manifold import MDS		def apply_mds(geodesic_distances: np.ndarray, n_components: int) -> np.ndarray:	    embedding = MDS(n_components=n_components)	    reduced_data = embedding.fit_transform(geodesic_distances)	    return reduced_data		save_history()
2024-11-17-14-12-37 104 13 def isomap(data: np.ndarray, n_components: int, k: int) -> np.ndarray:	    knn_graph = construct_knn_graph(data, k)	    geodesic_distances = compute_geodesic_distances(knn_graph)	    reduced_data = apply_mds(geodesic_distances, n_components)	    return reduced_data		from sklearn.datasets import load_digits	from sklearn.manifold import Isomap	X, _ = load_digits(return_X_y=True)	data = X[:100]	embedding = Isomap(n_components=2)	print(embedding.fit_transform(data))	print(isomap(data, 2, 1))		save_history()
2024-11-17-14-12-37 104 14 def initialize_centroids(data: np.ndarray, k: int) -> np.ndarray:	    centroids = []	    num_samples = data.shape[0]	    for _ in range(k):	        rand_ind = np.random.choice(num_samples)	        rand_data_point = data[rand_ind]	        centroids.append(rand_data_point)	    centroids = np.array(centroids)	    return centroids		# dummies = np.arange(99)	# dummies.shape = (-1, 3)	# print(initialize_centroids(dummies, 5))		save_history()
2024-11-17-14-12-37 104 15 def assign_clusters(data: np.ndarray, centroids: np.ndarray) -> np.ndarray:	    num_clusters = centroids.shape[0]	    labels = []	    for data_point in data:	        best_centroid_ind = 0	        best_euclidean_distance = 1e9	        for i in range (num_clusters):	            centroid = centroids[i]	            euclidean_distance = sum((centroid_val - data_point_val) ** 2 for centroid_val, data_point_val in zip(centroid, data_point)) ** 0.5	            if (euclidean_distance < best_euclidean_distance):	                best_euclidean_distance = euclidean_distance	                best_centroid_ind = i	        labels.append(best_centroid_ind)	    labels = np.array(labels)	    return labels		# dummies = np.random.normal(scale=20, size=(100, 3))	# print(dummies)	# print(assign_clusters(dummies, initialize_centroids(dummies, 5)))		save_history()
2024-11-17-14-12-37 104 16 def update_centroids(data: np.ndarray, labels: np.ndarray, k: int) -> np.ndarray:	    clusters = {}	    for i in range(k):	        clusters[f"{i}"] = []	    for i, data_point in enumerate(data):	        clusters[f"{labels[i]}"].append(data_point)	    new_centroids = []	    for i in range(k):	        cluster = np.array(clusters[f"{i}"])	        new_centroids.append(np.array([np.mean(feature) for feature in cluster.T]))		    new_centroids = np.array(new_centroids)	    return new_centroids		# dummies = np.random.normal(scale=20, size=(100, 3))	# labels = assign_clusters(dummies, initialize_centroids(dummies, 5))	# print(update_centroids(dummies, labels, 5))		save_history()
2024-11-17-14-12-37 104 17 def kmeans(data: np.ndarray, k: int, max_iter: int = 100, tol: float = 1e-4) -> Tuple[np.ndarray, np.ndarray]:	    centroids = initialize_centroids(data, k)	    labels = assign_clusters(data, centroids)	    max_movement = 1e9	    while (tol < max_movement and max_iter > 0):	        old_centroids = centroids.copy()	        centroids = update_centroids(data, labels, k)	        movements = []	        for i in range(centroids.shape[0]):	            old_centroid = old_centroids[i]	            centroid = centroids[i]	            movements.append(sum((old_val - val) ** 2 for old_val, val in zip(old_centroid, centroid)) ** 0.5)	        max_movement = max(movements)	        labels = assign_clusters(data, centroids)	        max_iter -= 1	    return (centroids, labels)		# true_centroids = [[0, 0], [-5, -5], [5, 5]]	# clusters = [np.random.randn(100, 2) + true_centroid for true_centroid in true_centroids]	# data = np.vstack(clusters)	# print(kmeans(data, len(true_centroids)))		save_history()
